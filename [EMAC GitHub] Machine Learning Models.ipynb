{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Grid limits pose equity barriers for distributed energy resources\n",
    "## <center> Machine Learning Models\n",
    "### <center> Jenny Conde & Anna Brockway"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"toc_1\"></a>\n",
    "* [Data Importing](#bullet1)\n",
    "* [Data Editing](#bullet2)\n",
    "* [Random Forest Regressions](#bullet3)\n",
    "* [Random Forest Classifications](#bullet4)\n",
    "* [Linear Regression Models](#bullet5)\n",
    "* [Logistic Regression Models](#bullet7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Importing <a class=\"anchor\" id=\"bullet1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.utils.validation import column_or_1d\n",
    "from sklearn.utils.validation import (check_array, check_consistent_length, _num_samples)\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import SCE Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: These data addresses are specific to Jenny's computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCE_alldata = pd.read_csv('../../../Box/distribution_data/CA_feeders/circuitfiles/SCE_ICAalldemotrees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35529, 61)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCE_alldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCE_realdata = pd.read_csv('../../../Box/distribution_data/CA_feeders/circuitfiles/SCE_ICAalldemotrees_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31334, 61)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCE_realdata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGE_alldata = pd.read_csv('../../../Box/distribution_data/CA_feeders/circuitfiles/PGE_ICAalldemotrees.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24820, 58)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PGE_alldata.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "PGE_realdata = pd.read_csv('../../../Box/distribution_data/CA_feeders/circuitfiles/PGE_ICAalldemotrees_real.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22524, 58)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PGE_realdata.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Editing <a class=\"anchor\" id=\"bullet2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"toc_1\"></a>\n",
    "* [Data Importing](#bullet1)\n",
    "* [Data Editing](#bullet2)\n",
    "* [Random Forest Regressions](#bullet3)\n",
    "* [Random Forest Classifications](#bullet4)\n",
    "* [Linear Regression Models](#bullet5)\n",
    "* [Logistic Regression Models](#bullet7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Drop Irrelevant Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First see what columns are available in SCE's and PG&E's data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CircuitName', 'GEOID10', 'ICL_kWphh', 'ICPVOF_kWphh', 'ICPV_kWphh',\n",
       "       'ICUGOF_kWphh', 'ICUG_kWphh', 'ICL_kWphh_cap', 'ICPVOF_kWphh_cap',\n",
       "       'ICPV_kWphh_cap', 'ICUGOF_kWphh_cap', 'ICUG_kWphh_cap',\n",
       "       'ICPVOF_e_kWphh', 'ICPV_e_kWphh', 'ICUGOF_e_kWphh', 'ICUG_e_kWphh',\n",
       "       'ICPVOF_e_kWphh_cap', 'ICPV_e_kWphh_cap', 'ICUGOF_e_kWphh_cap',\n",
       "       'ICUG_e_kWphh_cap', 'CircVolt_kV', 'Length_m', 'Length_m_ctot',\n",
       "       'Phase_max', 'Phase_min', 'CLSmax', 'CLSmin', 'ResCust', 'Res_pct',\n",
       "       'Com_pct', 'Ind_pct', 'Agr_pct', 'Oth_pct', 'tothh_Cpoly', 'tothh_ctot',\n",
       "       'tothh_pct', 'tothh_perkm', 'SAIDI5yravg', 'urbanheat_pctl',\n",
       "       'ghi_kWhpm2day', 'tothh', 'racediversity', 'black_pct', 'asian_pct',\n",
       "       'nlxwhite_pct', 'latinx_pct', 'inc50kbelow_pct', 'inc150kplus_pct',\n",
       "       'medhhinc', 'edavgyrs', 'ownerocc_pct', 'singleunit_pct', 'unitsavg',\n",
       "       'medyrbuilt', 'hhdensity_fctr', 'hhdensity_hhsqkm', 'polexposure_pctl',\n",
       "       'polenvt_pctl', 'popsens_pctl', 'lingisolation_pctl', 'sb535disad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCE_realdata.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['CircuitName', 'GEOID10', 'ICL_kWphh', 'ICPVOF_kWphh', 'ICPV_kWphh',\n",
       "       'ICUGOF_kWphh', 'ICUG_kWphh', 'ICL_kWphh_cap', 'ICPVOF_kWphh_cap',\n",
       "       'ICPV_kWphh_cap', 'ICUGOF_kWphh_cap', 'ICUG_kWphh_cap',\n",
       "       'ICPVOF_e_kWphh', 'ICPV_e_kWphh', 'ICUGOF_e_kWphh', 'ICUG_e_kWphh',\n",
       "       'ICPVOF_e_kWphh_cap', 'ICPV_e_kWphh_cap', 'ICUGOF_e_kWphh_cap',\n",
       "       'ICUG_e_kWphh_cap', 'CircVolt_kV', 'Length_m', 'Length_m_ctot',\n",
       "       'ICA_pct', 'ResCust', 'Res_pct', 'Com_pct', 'Ind_pct', 'Agr_pct',\n",
       "       'Oth_pct', 'tothh_Cpoly', 'tothh_ctot', 'tothh_pct', 'tothh_perkm',\n",
       "       'SAIDI5yravg', 'urbanheat_pctl', 'ghi_kWhpm2day', 'tothh',\n",
       "       'racediversity', 'black_pct', 'asian_pct', 'nlxwhite_pct', 'latinx_pct',\n",
       "       'inc50kbelow_pct', 'inc150kplus_pct', 'medhhinc', 'edavgyrs',\n",
       "       'ownerocc_pct', 'singleunit_pct', 'unitsavg', 'medyrbuilt',\n",
       "       'hhdensity_fctr', 'hhdensity_hhsqkm', 'polexposure_pctl',\n",
       "       'polenvt_pctl', 'popsens_pctl', 'lingisolation_pctl', 'sb535disad'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PGE_realdata.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count number of NAs in each column\n",
    "\n",
    "Drop Irrelevant Columns:\n",
    "1. CircuitName\n",
    "2. GEOID10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CircuitName\n",
      "0\n",
      "\n",
      "GEOID10\n",
      "0\n",
      "\n",
      "ICL_kWphh\n",
      "0\n",
      "\n",
      "ICPVOF_kWphh\n",
      "0\n",
      "\n",
      "ICPV_kWphh\n",
      "0\n",
      "\n",
      "ICUGOF_kWphh\n",
      "0\n",
      "\n",
      "ICUG_kWphh\n",
      "0\n",
      "\n",
      "ICL_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPVOF_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPV_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUGOF_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUG_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPVOF_e_kWphh\n",
      "0\n",
      "\n",
      "ICPV_e_kWphh\n",
      "0\n",
      "\n",
      "ICUGOF_e_kWphh\n",
      "0\n",
      "\n",
      "ICUG_e_kWphh\n",
      "0\n",
      "\n",
      "ICPVOF_e_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPV_e_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUGOF_e_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUG_e_kWphh_cap\n",
      "0\n",
      "\n",
      "CircVolt_kV\n",
      "0\n",
      "\n",
      "Length_m\n",
      "0\n",
      "\n",
      "Length_m_ctot\n",
      "0\n",
      "\n",
      "Phase_max\n",
      "0\n",
      "\n",
      "Phase_min\n",
      "0\n",
      "\n",
      "CLSmax\n",
      "41\n",
      "\n",
      "CLSmin\n",
      "41\n",
      "\n",
      "ResCust\n",
      "19\n",
      "\n",
      "Res_pct\n",
      "0\n",
      "\n",
      "Com_pct\n",
      "0\n",
      "\n",
      "Ind_pct\n",
      "0\n",
      "\n",
      "Agr_pct\n",
      "0\n",
      "\n",
      "Oth_pct\n",
      "0\n",
      "\n",
      "tothh_Cpoly\n",
      "0\n",
      "\n",
      "tothh_ctot\n",
      "0\n",
      "\n",
      "tothh_pct\n",
      "0\n",
      "\n",
      "tothh_perkm\n",
      "0\n",
      "\n",
      "SAIDI5yravg\n",
      "152\n",
      "\n",
      "urbanheat_pctl\n",
      "0\n",
      "\n",
      "ghi_kWhpm2day\n",
      "0\n",
      "\n",
      "tothh\n",
      "0\n",
      "\n",
      "racediversity\n",
      "0\n",
      "\n",
      "black_pct\n",
      "0\n",
      "\n",
      "asian_pct\n",
      "0\n",
      "\n",
      "nlxwhite_pct\n",
      "0\n",
      "\n",
      "latinx_pct\n",
      "0\n",
      "\n",
      "inc50kbelow_pct\n",
      "0\n",
      "\n",
      "inc150kplus_pct\n",
      "0\n",
      "\n",
      "medhhinc\n",
      "613\n",
      "\n",
      "edavgyrs\n",
      "2\n",
      "\n",
      "ownerocc_pct\n",
      "0\n",
      "\n",
      "singleunit_pct\n",
      "0\n",
      "\n",
      "unitsavg\n",
      "0\n",
      "\n",
      "medyrbuilt\n",
      "506\n",
      "\n",
      "hhdensity_fctr\n",
      "0\n",
      "\n",
      "hhdensity_hhsqkm\n",
      "0\n",
      "\n",
      "polexposure_pctl\n",
      "0\n",
      "\n",
      "polenvt_pctl\n",
      "0\n",
      "\n",
      "popsens_pctl\n",
      "0\n",
      "\n",
      "lingisolation_pctl\n",
      "359\n",
      "\n",
      "sb535disad\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in SCE_realdata:\n",
    "    print(col)\n",
    "    print(SCE_realdata[col].isnull().values.sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CircuitName\n",
      "0\n",
      "\n",
      "GEOID10\n",
      "0\n",
      "\n",
      "ICL_kWphh\n",
      "0\n",
      "\n",
      "ICPVOF_kWphh\n",
      "0\n",
      "\n",
      "ICPV_kWphh\n",
      "0\n",
      "\n",
      "ICUGOF_kWphh\n",
      "0\n",
      "\n",
      "ICUG_kWphh\n",
      "0\n",
      "\n",
      "ICL_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPVOF_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPV_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUGOF_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUG_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPVOF_e_kWphh\n",
      "0\n",
      "\n",
      "ICPV_e_kWphh\n",
      "0\n",
      "\n",
      "ICUGOF_e_kWphh\n",
      "0\n",
      "\n",
      "ICUG_e_kWphh\n",
      "0\n",
      "\n",
      "ICPVOF_e_kWphh_cap\n",
      "0\n",
      "\n",
      "ICPV_e_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUGOF_e_kWphh_cap\n",
      "0\n",
      "\n",
      "ICUG_e_kWphh_cap\n",
      "0\n",
      "\n",
      "CircVolt_kV\n",
      "0\n",
      "\n",
      "Length_m\n",
      "0\n",
      "\n",
      "Length_m_ctot\n",
      "0\n",
      "\n",
      "ICA_pct\n",
      "0\n",
      "\n",
      "ResCust\n",
      "0\n",
      "\n",
      "Res_pct\n",
      "0\n",
      "\n",
      "Com_pct\n",
      "0\n",
      "\n",
      "Ind_pct\n",
      "0\n",
      "\n",
      "Agr_pct\n",
      "0\n",
      "\n",
      "Oth_pct\n",
      "0\n",
      "\n",
      "tothh_Cpoly\n",
      "0\n",
      "\n",
      "tothh_ctot\n",
      "0\n",
      "\n",
      "tothh_pct\n",
      "0\n",
      "\n",
      "tothh_perkm\n",
      "0\n",
      "\n",
      "SAIDI5yravg\n",
      "7392\n",
      "\n",
      "urbanheat_pctl\n",
      "0\n",
      "\n",
      "ghi_kWhpm2day\n",
      "0\n",
      "\n",
      "tothh\n",
      "0\n",
      "\n",
      "racediversity\n",
      "0\n",
      "\n",
      "black_pct\n",
      "0\n",
      "\n",
      "asian_pct\n",
      "0\n",
      "\n",
      "nlxwhite_pct\n",
      "0\n",
      "\n",
      "latinx_pct\n",
      "0\n",
      "\n",
      "inc50kbelow_pct\n",
      "0\n",
      "\n",
      "inc150kplus_pct\n",
      "0\n",
      "\n",
      "medhhinc\n",
      "714\n",
      "\n",
      "edavgyrs\n",
      "0\n",
      "\n",
      "ownerocc_pct\n",
      "0\n",
      "\n",
      "singleunit_pct\n",
      "0\n",
      "\n",
      "unitsavg\n",
      "0\n",
      "\n",
      "medyrbuilt\n",
      "226\n",
      "\n",
      "hhdensity_fctr\n",
      "0\n",
      "\n",
      "hhdensity_hhsqkm\n",
      "0\n",
      "\n",
      "polexposure_pctl\n",
      "0\n",
      "\n",
      "polenvt_pctl\n",
      "0\n",
      "\n",
      "popsens_pctl\n",
      "0\n",
      "\n",
      "lingisolation_pctl\n",
      "408\n",
      "\n",
      "sb535disad\n",
      "0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for col in PGE_realdata:\n",
    "    print(col)\n",
    "    print(PGE_realdata[col].isnull().values.sum())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCE_realdata_dropped = SCE_realdata.drop(columns = ['CircuitName', 'GEOID10'])\n",
    "PGE_realdata_dropped = PGE_realdata.drop(columns = ['CircuitName', 'GEOID10'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One hot encode variables\n",
    "\n",
    "1. sb535disad: Should be a logical variable, drop one of the columns after one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31334, 62)\n",
      "(31334, 61)\n",
      "(22524, 59)\n",
      "(22524, 58)\n"
     ]
    }
   ],
   "source": [
    "# SCE\n",
    "SCE_realdata_encoded = pd.get_dummies(SCE_realdata_dropped)\n",
    "print(SCE_realdata_encoded.shape)\n",
    "# SCE_realdata_encoded = SCE_realdata_encoded.dropna()\n",
    "# Drop one of the categorical variables indicating disadvantages status\n",
    "SCE_realdata_encoded = SCE_realdata_encoded.drop(columns = 'sb535disad_No')\n",
    "print(SCE_realdata_encoded.shape)\n",
    "\n",
    "# PG&E\n",
    "PGE_realdata_encoded = pd.get_dummies(PGE_realdata_dropped)\n",
    "print(PGE_realdata_encoded.shape)\n",
    "# PGE_realdata_encoded = PGE_realdata_encoded.dropna()\n",
    "# Drop one of the categorical variables indicating disadvantages status\n",
    "PGE_realdata_encoded = PGE_realdata_encoded.drop(columns = 'sb535disad_No')\n",
    "print(PGE_realdata_encoded.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ICL_kWphh', 'ICPVOF_kWphh', 'ICPV_kWphh', 'ICUGOF_kWphh', 'ICUG_kWphh',\n",
       "       'ICL_kWphh_cap', 'ICPVOF_kWphh_cap', 'ICPV_kWphh_cap',\n",
       "       'ICUGOF_kWphh_cap', 'ICUG_kWphh_cap', 'ICPVOF_e_kWphh', 'ICPV_e_kWphh',\n",
       "       'ICUGOF_e_kWphh', 'ICUG_e_kWphh', 'ICPVOF_e_kWphh_cap',\n",
       "       'ICPV_e_kWphh_cap', 'ICUGOF_e_kWphh_cap', 'ICUG_e_kWphh_cap',\n",
       "       'CircVolt_kV', 'Length_m', 'Length_m_ctot', 'Phase_max', 'Phase_min',\n",
       "       'CLSmax', 'CLSmin', 'ResCust', 'Res_pct', 'Com_pct', 'Ind_pct',\n",
       "       'Agr_pct', 'Oth_pct', 'tothh_Cpoly', 'tothh_ctot', 'tothh_pct',\n",
       "       'tothh_perkm', 'SAIDI5yravg', 'urbanheat_pctl', 'ghi_kWhpm2day',\n",
       "       'tothh', 'racediversity', 'black_pct', 'asian_pct', 'nlxwhite_pct',\n",
       "       'latinx_pct', 'inc50kbelow_pct', 'inc150kplus_pct', 'medhhinc',\n",
       "       'edavgyrs', 'ownerocc_pct', 'singleunit_pct', 'unitsavg', 'medyrbuilt',\n",
       "       'hhdensity_hhsqkm', 'polexposure_pctl', 'polenvt_pctl', 'popsens_pctl',\n",
       "       'lingisolation_pctl', 'hhdensity_fctr_rural', 'hhdensity_fctr_suburban',\n",
       "       'hhdensity_fctr_urban', 'sb535disad_Yes'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SCE_realdata_encoded.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(31334, 9)\n",
      "(22524, 9)\n"
     ]
    }
   ],
   "source": [
    "y_SCE = SCE_realdata_encoded[['ICL_kWphh_cap', 'ICPVOF_kWphh_cap', 'ICPV_kWphh_cap', 'ICUGOF_kWphh_cap', 'ICUG_kWphh_cap',\n",
    "                             'ICPVOF_e_kWphh_cap', 'ICPV_e_kWphh_cap', 'ICUGOF_e_kWphh_cap', 'ICUG_e_kWphh_cap']]\n",
    "# print(y_SCE[y_SCE['ICL_kWphh_cap'].isnull()])\n",
    "# y_SCE = y_SCE.dropna()\n",
    "print(y_SCE.shape)\n",
    "y_PGE = PGE_realdata_encoded[['ICL_kWphh_cap', 'ICPVOF_kWphh_cap', 'ICPV_kWphh_cap', 'ICUGOF_kWphh_cap', 'ICUG_kWphh_cap',\n",
    "                             'ICPVOF_e_kWphh_cap', 'ICPV_e_kWphh_cap', 'ICUGOF_e_kWphh_cap', 'ICUG_e_kWphh_cap']]\n",
    "print(y_PGE.shape)\n",
    "# y_PCE = y_PGE.dropna()\n",
    "# print(y_PGE.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create sets of x variables/column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCE Independent Variable Sets\n",
    "\n",
    "X_SCE_infra_cols = ['CircVolt_kV', 'Phase_max', 'Phase_min', 'CLSmax', 'CLSmin', \n",
    "                    'Length_m', 'Length_m_ctot']\n",
    "\n",
    "X_SCE_service_cols = ['Res_pct', 'Com_pct', 'Ind_pct', 'Agr_pct', 'Oth_pct', 'tothh_Cpoly',\n",
    "                        'tothh_ctot', 'tothh_pct', 'tothh_perkm', 'ResCust', 'urbanheat_pctl', \n",
    "                        'ghi_kWhpm2day', 'SAIDI5yravg']\n",
    "\n",
    "X_SCE_demo_cols = ['tothh', 'racediversity', 'black_pct', 'asian_pct', 'nlxwhite_pct', \n",
    "                   'latinx_pct', 'inc50kbelow_pct', 'inc150kplus_pct', 'medhhinc', 'edavgyrs',\n",
    "                   'hhdensity_hhsqkm', 'ownerocc_pct', 'singleunit_pct', 'unitsavg', 'medyrbuilt',\n",
    "                   'polexposure_pctl', 'polenvt_pctl', 'popsens_pctl',\n",
    "                   'lingisolation_pctl', 'sb535disad_Yes']\n",
    "\n",
    "# X_SCE_infrademo_demo_cols = X_SCE_infrademo_cols + X_SCE_demo_cols\n",
    "X_SCE_all_cols = X_SCE_infra_cols + X_SCE_service_cols + X_SCE_demo_cols\n",
    "\n",
    "\n",
    "# PG&E Independent Variable Sets\n",
    "\n",
    "X_PGE_infra_cols = ['CircVolt_kV', 'Length_m', 'ICA_pct', \n",
    "                    'Length_m_ctot']\n",
    "\n",
    "X_PGE_service_cols = ['Res_pct', 'Com_pct', 'Ind_pct', 'Agr_pct', 'Oth_pct', 'tothh_Cpoly', \n",
    "                        'tothh_ctot', 'tothh_pct', 'tothh_perkm', 'ResCust', 'urbanheat_pctl', \n",
    "                        'ghi_kWhpm2day', 'SAIDI5yravg']\n",
    "\n",
    "X_PGE_demo_cols = ['tothh', 'racediversity', 'black_pct', 'asian_pct', 'nlxwhite_pct', \n",
    "                   'latinx_pct', 'inc50kbelow_pct', 'inc150kplus_pct', 'medhhinc', 'edavgyrs',\n",
    "                   'hhdensity_hhsqkm', 'ownerocc_pct', 'singleunit_pct', 'unitsavg', 'medyrbuilt', \n",
    "                   'polexposure_pctl', 'polenvt_pctl', 'popsens_pctl',\n",
    "                   'lingisolation_pctl', 'sb535disad_Yes']\n",
    "\n",
    "# X_PGE_infrademo_demo_cols = X_PGE_infrademo_cols + X_PGE_demo_cols\n",
    "X_PGE_all_cols = X_PGE_infra_cols + X_PGE_service_cols + X_PGE_demo_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create Train-Test Split -- this was ultimately not used because we decided it would be better to build our models with all data. The primary purpose of our analysis is to understand trends in the data, rather than using models for prediction. Therefore, we did not think a train-test split was needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train_SCE, X_test_SCE, y_train_SCE, y_test_SCE = train_test_split(SCE_realdata_encoded[X_SCE_all_cols], \n",
    "#                                                     y_SCE, \n",
    "#                                                     test_size = 0.1, random_state = 42)\n",
    "\n",
    "# X_train_PGE, X_test_PGE, y_train_PGE, y_test_PGE = train_test_split(PGE_realdata_encoded[X_PGE_all_cols], \n",
    "#                                                     y_PGE, \n",
    "#                                                     test_size = 0.1, random_state = 42)\n",
    "\n",
    "# test_size set to 90% of data, with these trees, we aren't too concerned with over-fitting the data\n",
    "# random_state set to 42 to keep consistency between runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create all the different X possibilities and y possibilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_train_SCE.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SCE\n",
    "\n",
    "# Split SCE_realdata_encoded according to columns\n",
    "X_all_SCE = SCE_realdata_encoded[X_SCE_all_cols].dropna()\n",
    "y_all_SCE = y_SCE.loc[X_all_SCE.index,:]\n",
    "X_infra_SCE = SCE_realdata_encoded[X_SCE_infra_cols].dropna()\n",
    "y_infra_SCE = y_SCE.loc[X_infra_SCE.index,:]\n",
    "X_service_SCE = SCE_realdata_encoded[X_SCE_service_cols].dropna()\n",
    "y_service_SCE = y_SCE.loc[X_service_SCE.index,:]\n",
    "# X_infrademo_demo_SCE = SCE_realdata_encoded[X_SCE_infrademo_demo_cols].dropna()\n",
    "# y_infrademo_demo_SCE = y_SCE.loc[X_infrademo_demo_SCE.index,:]\n",
    "X_demo_SCE = SCE_realdata_encoded[X_SCE_demo_cols].dropna()\n",
    "y_demo_SCE = y_SCE.loc[X_demo_SCE.index,:]\n",
    "\n",
    "# Run Train-Test Splits\n",
    "X_train_all_SCE, X_test_all_SCE, y_train_all_SCE, y_test_all_SCE = train_test_split(X_all_SCE, \n",
    "                                                    y_all_SCE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "X_train_infra_SCE, X_test_infra_SCE, y_train_infra_SCE, y_test_infra_SCE = train_test_split(X_infra_SCE, \n",
    "                                                    y_infra_SCE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "X_train_service_SCE, X_test_service_SCE, y_train_service_SCE, y_test_service_SCE = train_test_split(X_service_SCE, \n",
    "                                                    y_service_SCE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "X_train_demo_SCE, X_test_demo_SCE, y_train_demo_SCE, y_test_demo_SCE = train_test_split(X_demo_SCE, \n",
    "                                                    y_demo_SCE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "\n",
    "# test_size set to 90% of data, with these trees, we aren't too concerned with over-fitting the data\n",
    "# random_state set to 42 to keep consistency between runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(29718, 40)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_SCE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31293, 9)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_infra_SCE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR PG&E\n",
    "\n",
    "# Split PGE_realdata_encoded according to columns\n",
    "X_all_PGE = PGE_realdata_encoded[X_PGE_all_cols].dropna()\n",
    "y_all_PGE = y_PGE.loc[X_all_PGE.index,:]\n",
    "X_infra_PGE = PGE_realdata_encoded[X_PGE_infra_cols].dropna()\n",
    "y_infra_PGE = y_PGE.loc[X_infra_PGE.index,:]\n",
    "X_service_PGE = PGE_realdata_encoded[X_PGE_service_cols].dropna()\n",
    "y_service_PGE = y_PGE.loc[X_service_PGE.index,:]\n",
    "# X_infrademo_demo_PGE = PGE_realdata_encoded[X_PGE_infrademo_demo_cols].dropna()\n",
    "# y_infrademo_demo_PGE = y_PGE.loc[X_infrademo_demo_PGE.index,:]\n",
    "X_demo_PGE = PGE_realdata_encoded[X_PGE_demo_cols].dropna()\n",
    "y_demo_PGE = y_PGE.loc[X_demo_PGE.index,:]\n",
    "\n",
    "# Run Train-Test Splits\n",
    "X_train_all_PGE, X_test_all_PGE, y_train_all_PGE, y_test_all_PGE = train_test_split(X_all_PGE, \n",
    "                                                    y_all_PGE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "X_train_infra_PGE, X_test_infra_PGE, y_train_infra_PGE, y_test_infra_PGE = train_test_split(X_infra_PGE, \n",
    "                                                    y_infra_PGE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "X_train_service_PGE, X_test_service_PGE, y_train_service_PGE, y_test_service_PGE = train_test_split(X_service_PGE, \n",
    "                                                    y_service_PGE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "X_train_demo_PGE, X_test_demo_PGE, y_train_demo_PGE, y_test_demo_PGE = train_test_split(X_demo_PGE, \n",
    "                                                    y_demo_PGE, \n",
    "                                                    test_size = 0.1, random_state = 42)\n",
    "\n",
    "# test_size set to 90% of data, with these trees, we aren't too concerned with over-fitting the data\n",
    "# random_state set to 42 to keep consistency between runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14236, 37)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_all_PGE.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Regressions <a class=\"anchor\" id=\"bullet3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"toc_1\"></a>\n",
    "* [Data Importing](#bullet1)\n",
    "* [Data Editing](#bullet2)\n",
    "* [Random Forest Regressions](#bullet3)\n",
    "* [Random Forest Classifications](#bullet4)\n",
    "* [Linear Regression Models](#bullet5)\n",
    "* [Logistic Regression Models](#bullet7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regression Function\n",
    "\n",
    "We recreated some evaluation functions to account for weighting, which are featured above the primary regression function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_reg_targets(y_true, y_pred, multioutput, dtype=\"numeric\"):\n",
    "    \"\"\"Check that y_true and y_pred belong to the same regression task\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like\n",
    "    y_pred : array-like\n",
    "    multioutput : array-like or string in ['raw_values', uniform_average',\n",
    "        'variance_weighted'] or None\n",
    "        None is accepted due to backward compatibility of r2_score().\n",
    "    Returns\n",
    "    -------\n",
    "    type_true : one of {'continuous', continuous-multioutput'}\n",
    "        The type of the true target data, as output by\n",
    "        'utils.multiclass.type_of_target'\n",
    "    y_true : array-like of shape (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    multioutput : array-like of shape (n_outputs) or string in ['raw_values',\n",
    "        uniform_average', 'variance_weighted'] or None\n",
    "        Custom output weights if ``multioutput`` is array-like or\n",
    "        just the corresponding argument if ``multioutput`` is a\n",
    "        correct keyword.\n",
    "    dtype: str or list, default=\"numeric\"\n",
    "        the dtype argument passed to check_array\n",
    "    \"\"\"\n",
    "    check_consistent_length(y_true, y_pred)\n",
    "    y_true = check_array(y_true, ensure_2d=False, dtype=dtype)\n",
    "    y_pred = check_array(y_pred, ensure_2d=False, dtype=dtype)\n",
    "\n",
    "    if y_true.ndim == 1:\n",
    "        y_true = y_true.reshape((-1, 1))\n",
    "\n",
    "    if y_pred.ndim == 1:\n",
    "        y_pred = y_pred.reshape((-1, 1))\n",
    "\n",
    "    if y_true.shape[1] != y_pred.shape[1]:\n",
    "        raise ValueError(\"y_true and y_pred have different number of output \"\n",
    "                         \"({0}!={1})\".format(y_true.shape[1], y_pred.shape[1]))\n",
    "\n",
    "    n_outputs = y_true.shape[1]\n",
    "    allowed_multioutput_str = ('raw_values', 'uniform_average',\n",
    "                               'variance_weighted')\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput not in allowed_multioutput_str:\n",
    "            raise ValueError(\"Allowed 'multioutput' string values are {}. \"\n",
    "                             \"You provided multioutput={!r}\".format(\n",
    "                                 allowed_multioutput_str,\n",
    "                                 multioutput))\n",
    "    elif multioutput is not None:\n",
    "        multioutput = check_array(multioutput, ensure_2d=False)\n",
    "        if n_outputs == 1:\n",
    "            raise ValueError(\"Custom weights are useful only in \"\n",
    "                             \"multi-output cases.\")\n",
    "        elif n_outputs != len(multioutput):\n",
    "            raise ValueError((\"There must be equally many custom weights \"\n",
    "                              \"(%d) as outputs (%d).\") %\n",
    "                             (len(multioutput), n_outputs))\n",
    "    y_type = 'continuous' if n_outputs == 1 else 'continuous-multioutput'\n",
    "\n",
    "    return y_type, y_true, y_pred, multioutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_weighted_SCE(model, X_test, y_true, *,\n",
    "                       sample_weight=None,\n",
    "                       multioutput='uniform_average', squared=True):\n",
    "    \"\"\"Mean squared error regression loss\n",
    "    Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,), optional\n",
    "        Sample weights.\n",
    "    multioutput : string in ['raw_values', 'uniform_average'] \\\n",
    "                or array-like of shape (n_outputs)\n",
    "        Defines aggregating of multiple output values.\n",
    "        Array-like value defines weights used to average errors.\n",
    "        'raw_values' :\n",
    "            Returns a full set of errors in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Errors of all outputs are averaged with uniform weight.\n",
    "    squared : boolean value, optional (default = True)\n",
    "        If True returns MSE value, if False returns RMSE value.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float or ndarray of floats\n",
    "        A non-negative floating point value (the best value is 0.0), or an\n",
    "        array of floating point values, one for each individual target.\n",
    "    \"\"\"\n",
    "    sample_weight = SCE_realdata_encoded.loc[X_test.index.tolist(),'tothh_Cpoly']\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "        y_true, y_pred, multioutput)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
    "                               weights=sample_weight)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors if squared else np.sqrt(output_errors)\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    mse = np.average(output_errors, weights=multioutput)\n",
    "    return mse if squared else np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_squared_error_weighted_PGE(model, X_test, y_true, *,\n",
    "                       sample_weight=None,\n",
    "                       multioutput='uniform_average', squared=True):\n",
    "    \"\"\"Mean squared error regression loss\n",
    "    Read more in the :ref:`User Guide <mean_squared_error>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,), optional\n",
    "        Sample weights.\n",
    "    multioutput : string in ['raw_values', 'uniform_average'] \\\n",
    "                or array-like of shape (n_outputs)\n",
    "        Defines aggregating of multiple output values.\n",
    "        Array-like value defines weights used to average errors.\n",
    "        'raw_values' :\n",
    "            Returns a full set of errors in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Errors of all outputs are averaged with uniform weight.\n",
    "    squared : boolean value, optional (default = True)\n",
    "        If True returns MSE value, if False returns RMSE value.\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float or ndarray of floats\n",
    "        A non-negative floating point value (the best value is 0.0), or an\n",
    "        array of floating point values, one for each individual target.\n",
    "    \"\"\"\n",
    "    sample_weight = PGE_realdata_encoded.loc[X_test.index.tolist(),'tothh_Cpoly']\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "        y_true, y_pred, multioutput)\n",
    "    check_consistent_length(y_true, y_pred, sample_weight)\n",
    "    output_errors = np.average((y_true - y_pred) ** 2, axis=0,\n",
    "                               weights=sample_weight)\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            return output_errors if squared else np.sqrt(output_errors)\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # pass None as weights to np.average: uniform mean\n",
    "            multioutput = None\n",
    "\n",
    "    mse = np.average(output_errors, weights=multioutput)\n",
    "    return mse if squared else np.sqrt(mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_weighted_SCE(model, X_test, y_true, *, sample_weight = None,\n",
    "             multioutput=\"uniform_average\"):\n",
    "    \"\"\"R^2 (coefficient of determination) regression score function.\n",
    "    Best possible score is 1.0 and it can be negative (because the\n",
    "    model can be arbitrarily worse). A constant model that always\n",
    "    predicts the expected value of y, disregarding the input features,\n",
    "    would get a R^2 score of 0.0.\n",
    "    Read more in the :ref:`User Guide <r2_score>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,), optional\n",
    "        Sample weights.\n",
    "    multioutput : string in ['raw_values', 'uniform_average', \\\n",
    "'variance_weighted'] or None or array-like of shape (n_outputs)\n",
    "        Defines aggregating of multiple output scores.\n",
    "        Array-like value defines weights used to average scores.\n",
    "        Default is \"uniform_average\".\n",
    "        'raw_values' :\n",
    "            Returns a full set of scores in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Scores of all outputs are averaged with uniform weight.\n",
    "        'variance_weighted' :\n",
    "            Scores of all outputs are averaged, weighted by the variances\n",
    "            of each individual output.\n",
    "        .. versionchanged:: 0.19\n",
    "            Default value of multioutput is 'uniform_average'.\n",
    "    Returns\n",
    "    -------\n",
    "    z : float or ndarray of floats\n",
    "        The R^2 score or ndarray of scores if 'multioutput' is\n",
    "        'raw_values'.\n",
    "    Notes\n",
    "    -----\n",
    "    This is not a symmetric function.\n",
    "    Unlike most other scores, R^2 score may be negative (it need not actually\n",
    "    be the square of a quantity R).\n",
    "    This metric is not well-defined for single samples and will return a NaN\n",
    "    value if n_samples is less than two.\n",
    "    \"\"\"\n",
    "#     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "#         y_true, y_pred, multioutput)\n",
    "#     check_consistent_length(y_true, y_pred, sample_weight)\n",
    "\n",
    "#     print(X_test.columns)\n",
    "#     sample_weight = X_test['tothh_Cpoly']\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    sample_weight = SCE_realdata_encoded.loc[X_test.index.tolist(),'tothh_Cpoly']\n",
    "#     print(sample_weight)\n",
    "#     print()\n",
    "    \n",
    "    if _num_samples(y_pred) < 2:\n",
    "        msg = \"R^2 score is not well-defined with less than two samples.\"\n",
    "        warnings.warn(msg, UndefinedMetricWarning)\n",
    "        return float('nan')\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = column_or_1d(sample_weight)\n",
    "        weight = sample_weight[:, np.newaxis]\n",
    "        weight = weight.flatten()\n",
    "    else:\n",
    "        weight = 1.\n",
    "        \n",
    "#     print(weight)\n",
    "\n",
    "    numerator = np.sum(weight * (y_true - y_pred) ** 2, axis = 0)\n",
    "    denominator = np.sum(weight * (y_true - np.average(\n",
    "        y_true, axis=0, weights=sample_weight)) ** 2, axis = 0)\n",
    "#     numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n",
    "#                                                       dtype=np.float64)\n",
    "#     denominator = (weight * (y_true - np.average(\n",
    "#         y_true, axis=0, weights=sample_weight)) ** 2).sum(axis=0,\n",
    "#                                                           dtype=np.float64)\n",
    "    nonzero_denominator = denominator != 0\n",
    "    nonzero_numerator = numerator != 0\n",
    "    valid_score = nonzero_denominator & nonzero_numerator\n",
    "    output_scores = np.ones([len(y_true)])\n",
    "    output_scores[valid_score] = 1 - (numerator[valid_score] /\n",
    "                                      denominator[valid_score])\n",
    "    # arbitrary set to zero to avoid -inf scores, having a constant\n",
    "    # y_true is not interesting for scoring a regression anyway\n",
    "    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            # return scores individually\n",
    "            return output_scores\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # passing None as weights results is uniform mean\n",
    "            avg_weights = None\n",
    "        elif multioutput == 'variance_weighted':\n",
    "            avg_weights = denominator\n",
    "            # avoid fail on constant y or one-element arrays\n",
    "            if not np.any(nonzero_denominator):\n",
    "                if not np.any(nonzero_numerator):\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    return 0.0\n",
    "    else:\n",
    "        avg_weights = multioutput\n",
    "\n",
    "    return np.average(output_scores, weights=avg_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def r2_score_weighted_PGE(model, X_test, y_true, *, sample_weight = None,\n",
    "             multioutput=\"uniform_average\"):\n",
    "    \"\"\"R^2 (coefficient of determination) regression score function.\n",
    "    Best possible score is 1.0 and it can be negative (because the\n",
    "    model can be arbitrarily worse). A constant model that always\n",
    "    predicts the expected value of y, disregarding the input features,\n",
    "    would get a R^2 score of 0.0.\n",
    "    Read more in the :ref:`User Guide <r2_score>`.\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Ground truth (correct) target values.\n",
    "    y_pred : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "        Estimated target values.\n",
    "    sample_weight : array-like of shape (n_samples,), optional\n",
    "        Sample weights.\n",
    "    multioutput : string in ['raw_values', 'uniform_average', \\\n",
    "'variance_weighted'] or None or array-like of shape (n_outputs)\n",
    "        Defines aggregating of multiple output scores.\n",
    "        Array-like value defines weights used to average scores.\n",
    "        Default is \"uniform_average\".\n",
    "        'raw_values' :\n",
    "            Returns a full set of scores in case of multioutput input.\n",
    "        'uniform_average' :\n",
    "            Scores of all outputs are averaged with uniform weight.\n",
    "        'variance_weighted' :\n",
    "            Scores of all outputs are averaged, weighted by the variances\n",
    "            of each individual output.\n",
    "        .. versionchanged:: 0.19\n",
    "            Default value of multioutput is 'uniform_average'.\n",
    "    Returns\n",
    "    -------\n",
    "    z : float or ndarray of floats\n",
    "        The R^2 score or ndarray of scores if 'multioutput' is\n",
    "        'raw_values'.\n",
    "    Notes\n",
    "    -----\n",
    "    This is not a symmetric function.\n",
    "    Unlike most other scores, R^2 score may be negative (it need not actually\n",
    "    be the square of a quantity R).\n",
    "    This metric is not well-defined for single samples and will return a NaN\n",
    "    value if n_samples is less than two.\n",
    "    \"\"\"\n",
    "#     y_type, y_true, y_pred, multioutput = _check_reg_targets(\n",
    "#         y_true, y_pred, multioutput)\n",
    "#     check_consistent_length(y_true, y_pred, sample_weight)\n",
    "\n",
    "#     print(X_test.columns)\n",
    "#     sample_weight = X_test['tothh_Cpoly']\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    sample_weight = PGE_realdata_encoded.loc[X_test.index.tolist(),'tothh_Cpoly']\n",
    "#     print(sample_weight)\n",
    "#     print()\n",
    "    \n",
    "    if _num_samples(y_pred) < 2:\n",
    "        msg = \"R^2 score is not well-defined with less than two samples.\"\n",
    "        warnings.warn(msg, UndefinedMetricWarning)\n",
    "        return float('nan')\n",
    "\n",
    "    if sample_weight is not None:\n",
    "        sample_weight = column_or_1d(sample_weight)\n",
    "        weight = sample_weight[:, np.newaxis]\n",
    "        weight = weight.flatten()\n",
    "    else:\n",
    "        weight = 1.\n",
    "        \n",
    "#     print(weight)\n",
    "\n",
    "    numerator = np.sum(weight * (y_true - y_pred) ** 2, axis = 0)\n",
    "    denominator = np.sum(weight * (y_true - np.average(\n",
    "        y_true, axis=0, weights=sample_weight)) ** 2, axis = 0)\n",
    "#     numerator = (weight * (y_true - y_pred) ** 2).sum(axis=0,\n",
    "#                                                       dtype=np.float64)\n",
    "#     denominator = (weight * (y_true - np.average(\n",
    "#         y_true, axis=0, weights=sample_weight)) ** 2).sum(axis=0,\n",
    "#                                                           dtype=np.float64)\n",
    "    nonzero_denominator = denominator != 0\n",
    "    nonzero_numerator = numerator != 0\n",
    "    valid_score = nonzero_denominator & nonzero_numerator\n",
    "    output_scores = np.ones([len(y_true)])\n",
    "    output_scores[valid_score] = 1 - (numerator[valid_score] /\n",
    "                                      denominator[valid_score])\n",
    "    # arbitrary set to zero to avoid -inf scores, having a constant\n",
    "    # y_true is not interesting for scoring a regression anyway\n",
    "    output_scores[nonzero_numerator & ~nonzero_denominator] = 0.\n",
    "    if isinstance(multioutput, str):\n",
    "        if multioutput == 'raw_values':\n",
    "            # return scores individually\n",
    "            return output_scores\n",
    "        elif multioutput == 'uniform_average':\n",
    "            # passing None as weights results is uniform mean\n",
    "            avg_weights = None\n",
    "        elif multioutput == 'variance_weighted':\n",
    "            avg_weights = denominator\n",
    "            # avoid fail on constant y or one-element arrays\n",
    "            if not np.any(nonzero_denominator):\n",
    "                if not np.any(nonzero_numerator):\n",
    "                    return 1.0\n",
    "                else:\n",
    "                    return 0.0\n",
    "    else:\n",
    "        avg_weights = multioutput\n",
    "\n",
    "    return np.average(output_scores, weights=avg_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function with Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_regression_ttsplit(y, run, X_train, X_test, y_train, y_test, utility, lessthan10):\n",
    "    \"\"\"\n",
    "    Run the Random Forest Regression Models\n",
    "    Find the best value for max_features using cross validation\n",
    "    Save figure comparing CV Scores\n",
    "    Save CSVs of feature importances and errors\n",
    "    \n",
    "    y options:\n",
    "    0. ICL_kWphh\n",
    "    1. ICPVOF_kWphh\n",
    "    2. ICPV_kWphh\n",
    "    3. ICUGOF_kWphh\n",
    "    4. ICUG_kWphh\n",
    "    \n",
    "    Run options:\n",
    "    0. all variables\n",
    "    1. infra variables\n",
    "    2. infrademo_demo variables\n",
    "    3. demo variables\n",
    "    \"\"\"\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    if '_cap' in y:\n",
    "        y_mod = y.replace('_cap', '')\n",
    "        \n",
    "    if lessthan10:\n",
    "        y_train = y_train.where(y_train < 10).dropna()\n",
    "        X_train = X_train.loc[y_train.index,:]\n",
    "        y_test = y_train.where(y_train < 10).dropna()\n",
    "        X_test = X_train.loc[y_train.index,:]\n",
    "    \n",
    "    # Determine best model for max_features\n",
    "    rf_model_onethird = RandomForestRegressor(max_features = 1/3)\n",
    "    rf_model_sqrt = RandomForestRegressor(max_features = 'sqrt')\n",
    "    rf_model_n_features = RandomForestRegressor()\n",
    "    \n",
    "    # Perform Cross Validation (depends on utility)\n",
    "    # Also define other variables that are utility-dependent (ex. when saving CSVs)\n",
    "    if utility == 'SCE':\n",
    "        weights = SCE_realdata_encoded.loc[X_train.index, 'tothh_Cpoly']\n",
    "        test_weights = SCE_realdata_encoded.loc[X_test.index, 'tothh_Cpoly']\n",
    "        \n",
    "#         print(X_train.isnull().any().any())\n",
    "#         print(y_train.isnull().any())\n",
    "        \n",
    "#         one_third_r2 = np.mean(cross_val_score(rf_model_onethird, X_train, y_train, \n",
    "#                                               fit_params = {'sample_weight' : weights},\n",
    "#                                               cv = 10, scoring = r2_score_weighted_SCE))\n",
    "#         sqrt_r2 = np.mean(cross_val_score(rf_model_sqrt, X_train, y_train, \n",
    "#                                          fit_params = {'sample_weight' : weights},\n",
    "#                                          cv = 10, scoring = r2_score_weighted_SCE))\n",
    "#         n_features_r2 = np.mean(cross_val_score(rf_model_n_features, X_train, y_train, \n",
    "#                                                fit_params = {'sample_weight' : weights},\n",
    "#                                                cv = 10, scoring = r2_score_weighted_SCE))\n",
    "        one_third_mse = np.mean(cross_val_score(rf_model_onethird, X_train, y_train, \n",
    "                                              fit_params = {'sample_weight' : weights},\n",
    "                                              cv = 10, scoring = mean_squared_error_weighted_SCE))\n",
    "        sqrt_mse = np.mean(cross_val_score(rf_model_sqrt, X_train, y_train, \n",
    "                                         fit_params = {'sample_weight' : weights},\n",
    "                                         cv = 10, scoring = mean_squared_error_weighted_SCE))\n",
    "        n_features_mse = np.mean(cross_val_score(rf_model_n_features, X_train, y_train, \n",
    "                                               fit_params = {'sample_weight' : weights},\n",
    "                                               cv = 10, scoring = mean_squared_error_weighted_SCE))\n",
    "        if lessthan10:\n",
    "            cv_scores_col_name = 'SCE CV Scores for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_scores_df_name = 'SCE_cv_' + y + '_' + run + '_10.csv'\n",
    "            FI_col_name = 'SCE FI for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            fi_df_name = 'SCE_FeatImp_' + y + '_' + run + '_10.csv'\n",
    "            fi_fig_name = 'SCE_FI_' + y + '_' + run + '_10.png'\n",
    "            eval_col_name = 'SCE Eval metrics for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            eval_df_name = 'SCE_Eval_' + y + '_' + run + '_10.csv'\n",
    "            cv_title = 'SCE Cross Validation Results for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_r2_name = 'SCE_cv_' + y + '_' + run + '_r2_10.png'\n",
    "            cv_mse_name = 'SCE_cv_' + y + '_' + run + '_mse_10.png'\n",
    "        else:\n",
    "            cv_scores_col_name = 'SCE CV Scores for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            cv_scores_df_name = 'SCE_cv_' + y + '_' + run + '.csv'\n",
    "            FI_col_name = 'SCE FI for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            fi_df_name = 'SCE_FeatImp_' + y + '_' + run + '.csv'\n",
    "            fi_fig_name = 'SCE_FI_' + y + '_' + run + '.png'\n",
    "            eval_col_name = 'SCE Eval metrics for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            eval_df_name = 'SCE_Eval_' + y + '_' + run + '.csv'\n",
    "            cv_title = 'SCE Cross Validation Results for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            cv_r2_name = 'SCE_cv_' + y + '_' + run + '_r2.png'\n",
    "            cv_mse_name = 'SCE_cv_' + y + '_' + run + '_mse.png'\n",
    "    else: # utility == 'PGE'\n",
    "        weights = PGE_realdata_encoded.loc[X_train.index, 'tothh_Cpoly']\n",
    "        test_weights = PGE_realdata_encoded.loc[X_test.index, 'tothh_Cpoly']\n",
    "        \n",
    "        print(X_train.isnull().any().any())\n",
    "        print(y_train.isnull().any())\n",
    "        \n",
    "        one_third_r2 = np.mean(cross_val_score(rf_model_onethird, X_train, y_train, \n",
    "                                              fit_params = {'sample_weight' : weights},\n",
    "                                              cv = 10, scoring = r2_score_weighted_PGE))\n",
    "        sqrt_r2 = np.mean(cross_val_score(rf_model_sqrt, X_train, y_train, \n",
    "                                         fit_params = {'sample_weight' : weights},\n",
    "                                         cv = 10, scoring = r2_score_weighted_PGE))\n",
    "        n_features_r2 = np.mean(cross_val_score(rf_model_n_features, X_train, y_train, \n",
    "                                               fit_params = {'sample_weight' : weights},\n",
    "                                               cv = 10, scoring = r2_score_weighted_PGE))\n",
    "        one_third_mse = np.mean(cross_val_score(rf_model_onethird, X_train, y_train, \n",
    "                                              fit_params = {'sample_weight' : weights},\n",
    "                                              cv = 10, scoring = mean_squared_error_weighted_PGE))\n",
    "        sqrt_mse = np.mean(cross_val_score(rf_model_sqrt, X_train, y_train, \n",
    "                                         fit_params = {'sample_weight' : weights},\n",
    "                                         cv = 10, scoring = mean_squared_error_weighted_PGE))\n",
    "        n_features_mse = np.mean(cross_val_score(rf_model_n_features, X_train, y_train, \n",
    "                                               fit_params = {'sample_weight' : weights},\n",
    "                                               cv = 10, scoring = mean_squared_error_weighted_PGE))\n",
    "        if lessthan10:\n",
    "            cv_scores_col_name = 'PG&E CV Scores for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_scores_df_name = 'PGE_cv_' + y + '_' + run + '_10.csv'\n",
    "            FI_col_name = 'PG&E FI for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            fi_df_name = 'PGE_FeatImp_' + y + '_' + run + '_10.csv'\n",
    "            fi_fig_name = 'PGE_FI_' + y + '_' + run + '_10.png'\n",
    "            eval_col_name = 'PG&E Eval metrics for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            eval_df_name = 'PGE_Eval_' + y + '_' + run + '_10.csv'\n",
    "            cv_title = 'PG&E Cross Validation Results for ' + y_mod + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_r2_name = 'PGE_cv_' + y + '_' + run + '_r2_10.png'\n",
    "            cv_mse_name = 'PGE_cv_' + y + '_' + run + '_mse_10.png'\n",
    "        else:\n",
    "            cv_scores_col_name = 'PG&E CV Scores for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            cv_scores_df_name = 'PGE_cv_' + y + '_' + run + '.csv'\n",
    "            FI_col_name = 'PG&E FI for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            fi_df_name = 'PGE_FeatImp_' + y + '_' + run + '.csv'\n",
    "            fi_fig_name = 'PGE_FI_' + y + '_' + run + '.png'\n",
    "            eval_col_name = 'PG&E Eval metrics for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            eval_df_name = 'PGE_Eval_' + y + '_' + run + '.csv'\n",
    "            cv_title = 'PG&E Cross Validation Results for ' + y_mod + ' with ' + run + ' Variables'\n",
    "            cv_r2_name = 'PGE_cv_' + y + '_' + run + '_r2.png'\n",
    "            cv_mse_name = 'PGE_cv_' + y + '_' + run + '_mse.png'\n",
    "        \n",
    "    \n",
    "    # Find Best Performing model (Maximum R^2 Value)\n",
    "#     cv_r2_scores = [one_third_r2, sqrt_r2, n_features_r2]\n",
    "    cv_mse_scores = [one_third_mse, sqrt_mse, n_features_mse]\n",
    "    cv_scores = cv_mse_scores # + cv_r2_scores\n",
    "    model_to_use = cv_scores.index(min(cv_scores)) # use model with min MSE\n",
    "    if model_to_use == 0:\n",
    "        rf_model = rf_model_onethird\n",
    "    elif model_to_use == 1:\n",
    "        rf_model = rf_model_sqrt\n",
    "    else: # model_to_use == 2\n",
    "        rf_model = rf_model_n_features\n",
    "    cv_scores_df = pd.DataFrame(cv_scores, index = ['one-third, mse', 'sqrt, mse', 'n_features, mse',], \n",
    "                                columns = [cv_scores_col_name])\n",
    "    save_to_cv_scores = cwd + '/cv_results/' + cv_scores_df_name\n",
    "    cv_scores_df.to_csv(save_to_cv_scores)\n",
    "    \n",
    "    # Plot Results\n",
    "#     plt.figure()\n",
    "#     plt.bar(x = ['one-third', 'sqrt', 'n_features'], height = cv_r2_scores);\n",
    "#     plt.xlabel('Value for max_features', fontsize = 13);\n",
    "#     plt.ylabel('Cross Validation $R^2$ Score', fontsize = 13)\n",
    "#     plt.title(cv_title)\n",
    "#     save_to_bar_chart = cwd + '/figures/' + cv_r2_name\n",
    "#     plt.savefig(save_to_bar_chart, dpi = 300, bbox_inches = 'tight');\n",
    "#     plt.close()\n",
    "    plt.figure()\n",
    "    plt.bar(x = ['one-third', 'sqrt', 'n_features'], height = cv_mse_scores);\n",
    "    plt.xlabel('Value for max_features', fontsize = 13);\n",
    "    plt.ylabel('Cross Validation MSE Score', fontsize = 13)\n",
    "    plt.title(cv_title)\n",
    "    save_to_bar_chart = cwd + '/figures/' + cv_mse_name\n",
    "    plt.savefig(save_to_bar_chart, dpi = 300, bbox_inches = 'tight');\n",
    "    plt.close()\n",
    "\n",
    "    # Fit Final Model\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    # Make predictions for both train and test sets\n",
    "    y_train_pred = rf_model.predict(X_train)\n",
    "    y_test_pred = rf_model.predict(X_test)\n",
    "    # Find Feature Importances\n",
    "    FI = rf_model.feature_importances_\n",
    "    # Evaluation metrics: weighted R^2 and weighted RMSE\n",
    "    r2_train = rf_model.score(X_train, y_train, sample_weight = weights)\n",
    "    r2_test = rf_model.score(X_test, y_test, sample_weight = test_weights)\n",
    "    mse_train = mean_squared_error(y_train_pred, y_train, sample_weight = weights)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    mse_test = mean_squared_error(y_test_pred, y_test, sample_weight = test_weights)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    \n",
    "    # Save Feature Importance Data\n",
    "    FI_df = pd.DataFrame(data = FI, index = X_train.columns, columns = [FI_col_name])\n",
    "    if 'hhdensity_fctr_rural' in X_train.columns:\n",
    "        # Aggregate with a sum\n",
    "        summed = FI_df.loc['hhdensity_fctr_rural',FI_col_name] + FI_df.loc['hhdensity_fctr_suburban',FI_col_name] + FI_df.loc['hhdensity_fctr_urban',FI_col_name]\n",
    "        \n",
    "        # Aggregate with a weighted average\n",
    "#         adequate_count = sum(X_train['DelivNote_Adequate'])\n",
    "#         inadequate_count = sum(X_train['DelivNote_Inadequate'])\n",
    "#         undet_count = sum(X_train['DelivNote_Undetermined'])\n",
    "#         w_avg = (FI_df.loc['DelivNote_Adequate',FI_col_name]*adequate_count + FI_df.loc['DelivNote_Inadequate',FI_col_name]*inadequate_count + FI_df.loc['DelivNote_Undetermined',FI_col_name]*undet_count)/sum([adequate_count, inadequate_count, undet_count])\n",
    "        \n",
    "        # FI_df.drop(index = ['DelivNote_Adequate', 'DelivNote_Inadequate', 'DelivNote_Undetermined'])\n",
    "        summed_col = FI_col_name + ', Summed'\n",
    "#         w_avg_col = FI_col_name + ', Weighted Avg'\n",
    "        FI_df[summed_col] = FI_df[FI_col_name]\n",
    "#         FI_df[w_avg_col] = FI_df[FI_col_name]\n",
    "        FI_df.loc['hhdensity_fctr',:] = [0, summed]\n",
    "        FI_df.loc[['hhdensity_fctr_rural', 'hhdensity_fctr_suburban', 'hhdensity_fctr_urban'], [summed_col]] = 0\n",
    "        FI_df[summed_col] = FI_df[summed_col] / sum(FI_df[summed_col])\n",
    "#         FI_df[w_avg_col] = FI_df[w_avg_col] / sum(FI_df[w_avg_col])\n",
    "    save_to_FI = cwd + '/feat_imp/' + fi_df_name\n",
    "    FI_df.to_csv(save_to_FI)\n",
    "    \n",
    "    # Save Feature Importance Data as Barchart\n",
    "    if 'hhdensity_fctr_rural' in X_train.columns:\n",
    "        primary_col = summed_col\n",
    "        primary_col_name = 'summed'\n",
    "        FI_df_sorted = FI_df.sort_values(by = primary_col, ascending = False)\n",
    "        FI_df_sorted = FI_df_sorted.rename(columns = {FI_col_name : 'normal',\n",
    "                                                 summed_col : 'summed'})\n",
    "        FI_df_sorted[primary_col_name].plot(kind = 'bar', figsize = (10,5))\n",
    "    else:\n",
    "        primary_col = FI_col_name\n",
    "        primary_col_name = 'normal'\n",
    "        FI_df_sorted = FI_df.sort_values(by = primary_col, ascending = False)\n",
    "        FI_df_sorted[primary_col].plot(kind = 'bar', figsize = (10,5))\n",
    "        \n",
    "    plt.xlabel('Features', fontsize = 15);\n",
    "    plt.ylabel('Feature Importance', fontsize = 15)\n",
    "    plt.title(FI_col_name, fontsize = 15)\n",
    "    save_to_fi_barchart = cwd + '/figures/' + fi_fig_name\n",
    "    plt.savefig(save_to_fi_barchart, dpi = 300, bbox_inches = 'tight');\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Save Evaluation Metrics Data\n",
    "    eval_df = pd.DataFrame(data = [r2_train, r2_test, mse_train, mse_test, rmse_train, rmse_test], \n",
    "                           index = ['r2_train', 'r2_test', 'mse_train', 'mse_test', 'rmse_train', 'rmse_test'],\n",
    "                                   columns = [eval_col_name])\n",
    "    save_to_eval = cwd + '/eval_metrics/' + eval_df_name\n",
    "    eval_df.to_csv(save_to_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function with no Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_regression(y_val, run, X, y, utility, lessthan10):\n",
    "    \"\"\"\n",
    "    Run the Random Forest Regression Models\n",
    "    Find the best value for max_features using cross validation\n",
    "    Save figure comparing CV Scores\n",
    "    Save CSVs of feature importances and errors\n",
    "    \n",
    "    y options:\n",
    "    0. ICL_kWphh\n",
    "    1. ICPVOF_kWphh\n",
    "    2. ICPV_kWphh\n",
    "    3. ICUGOF_kWphh\n",
    "    4. ICUG_kWphh\n",
    "    \n",
    "    Run options:\n",
    "    0. all variables\n",
    "    1. infra variables\n",
    "    2. infrademo_demo variables\n",
    "    3. demo variables\n",
    "    \"\"\"\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "#     if '_cap' in y_val:\n",
    "#         y_mod = y_val.replace('_cap', '')\n",
    "        \n",
    "    if lessthan10:\n",
    "        y = y.where(y < 10).dropna()\n",
    "        X = X.loc[y.index,:]\n",
    "    \n",
    "    # Determine best model for max_features\n",
    "    rf_model_onethird = RandomForestRegressor(max_features = 1/3)\n",
    "    rf_model_sqrt = RandomForestRegressor(max_features = 'sqrt')\n",
    "    rf_model_n_features = RandomForestRegressor()\n",
    "    \n",
    "    # Perform Cross Validation (depends on utility)\n",
    "    # Also define other variables that are utility-dependent (ex. when saving CSVs)\n",
    "    if utility == 'SCE':\n",
    "        weights = SCE_realdata_encoded.loc[X.index, 'tothh_Cpoly']\n",
    "#         test_weights = SCE_realdata_encoded.loc[X.index, 'tothh_Cpoly']\n",
    "        \n",
    "        one_third_mse = np.mean(cross_val_score(rf_model_onethird, X, y, \n",
    "                                              fit_params = {'sample_weight' : weights},\n",
    "                                              cv = 10, scoring = mean_squared_error_weighted_SCE))\n",
    "        sqrt_mse = np.mean(cross_val_score(rf_model_sqrt, X, y, \n",
    "                                         fit_params = {'sample_weight' : weights},\n",
    "                                         cv = 10, scoring = mean_squared_error_weighted_SCE))\n",
    "        n_features_mse = np.mean(cross_val_score(rf_model_n_features, X, y, \n",
    "                                               fit_params = {'sample_weight' : weights},\n",
    "                                               cv = 10, scoring = mean_squared_error_weighted_SCE))\n",
    "        if lessthan10:\n",
    "            cv_scores_col_name = 'SCE CV Scores for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_scores_df_name = 'SCE_cv_' + y_val + '_' + run + '_10.csv'\n",
    "            FI_col_name = 'SCE FI for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            fi_df_name = 'SCE_FeatImp_' + y_val + '_' + run + '_10.csv'\n",
    "            fi_fig_name = 'SCE_FI_' + y_val + '_' + run + '_10.png'\n",
    "            eval_col_name = 'SCE Eval metrics for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            eval_df_name = 'SCE_Eval_' + y_val + '_' + run + '_10.csv'\n",
    "            cv_title = 'SCE Cross Validation Results for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_r2_name = 'SCE_cv_' + y_val + '_' + run + '_r2_10.png'\n",
    "            cv_mse_name = 'SCE_cv_' + y_val + '_' + run + '_mse_10.png'\n",
    "        else:\n",
    "            cv_scores_col_name = 'SCE CV Scores for ' + y_val + ' with ' + run + ' Variables'\n",
    "            cv_scores_df_name = 'SCE_cv_' + y_val + '_' + run + '.csv'\n",
    "            FI_col_name = 'SCE FI for ' + y_val + ' with ' + run + ' Variables'\n",
    "            fi_df_name = 'SCE_FeatImp_' + y_val + '_' + run + '.csv'\n",
    "            fi_fig_name = 'SCE_FI_' + y_val + '_' + run + '.png'\n",
    "            eval_col_name = 'SCE Eval metrics for ' + y_val + ' with ' + run + ' Variables'\n",
    "            eval_df_name = 'SCE_Eval_' + y_val + '_' + run + '.csv'\n",
    "            cv_title = 'SCE Cross Validation Results for ' + y_val + ' with ' + run + ' Variables'\n",
    "            cv_r2_name = 'SCE_cv_' + y_val + '_' + run + '_r2.png'\n",
    "            cv_mse_name = 'SCE_cv_' + y_val + '_' + run + '_mse.png'\n",
    "    else: # utility == 'PGE'\n",
    "        weights = PGE_realdata_encoded.loc[X.index, 'tothh_Cpoly']\n",
    "#         test_weights = PGE_realdata_encoded.loc[X_test.index, 'tothh_Cpoly']\n",
    "\n",
    "        one_third_mse = np.mean(cross_val_score(rf_model_onethird, X, y, \n",
    "                                              fit_params = {'sample_weight' : weights},\n",
    "                                              cv = 10, scoring = mean_squared_error_weighted_PGE))\n",
    "        sqrt_mse = np.mean(cross_val_score(rf_model_sqrt, X, y, \n",
    "                                         fit_params = {'sample_weight' : weights},\n",
    "                                         cv = 10, scoring = mean_squared_error_weighted_PGE))\n",
    "        n_features_mse = np.mean(cross_val_score(rf_model_n_features, X, y, \n",
    "                                               fit_params = {'sample_weight' : weights},\n",
    "                                               cv = 10, scoring = mean_squared_error_weighted_PGE))\n",
    "        if lessthan10:\n",
    "            cv_scores_col_name = 'PG&E CV Scores for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_scores_df_name = 'PGE_cv_' + y_val + '_' + run + '_10.csv'\n",
    "            FI_col_name = 'PG&E FI for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            fi_df_name = 'PGE_FeatImp_' + y_val + '_' + run + '_10.csv'\n",
    "            fi_fig_name = 'PGE_FI_' + y_val + '_' + run + '_10.png'\n",
    "            eval_col_name = 'PG&E Eval metrics for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            eval_df_name = 'PGE_Eval_' + y_val + '_' + run + '_10.csv'\n",
    "            cv_title = 'PG&E Cross Validation Results for ' + y_val + ' with ' + run + ' Variables, <10kW/hh'\n",
    "            cv_r2_name = 'PGE_cv_' + y_val + '_' + run + '_r2_10.png'\n",
    "            cv_mse_name = 'PGE_cv_' + y_val + '_' + run + '_mse_10.png'\n",
    "        else:\n",
    "            cv_scores_col_name = 'PG&E CV Scores for ' + y_val + ' with ' + run + ' Variables'\n",
    "            cv_scores_df_name = 'PGE_cv_' + y_val + '_' + run + '.csv'\n",
    "            FI_col_name = 'PG&E FI for ' + y_val + ' with ' + run + ' Variables'\n",
    "            fi_df_name = 'PGE_FeatImp_' + y_val + '_' + run + '.csv'\n",
    "            fi_fig_name = 'PGE_FI_' + y_val + '_' + run + '.png'\n",
    "            eval_col_name = 'PG&E Eval metrics for ' + y_val + ' with ' + run + ' Variables'\n",
    "            eval_df_name = 'PGE_Eval_' + y_val + '_' + run + '.csv'\n",
    "            cv_title = 'PG&E Cross Validation Results for ' + y_val + ' with ' + run + ' Variables'\n",
    "            cv_r2_name = 'PGE_cv_' + y_val + '_' + run + '_r2.png'\n",
    "            cv_mse_name = 'PGE_cv_' + y_val + '_' + run + '_mse.png'\n",
    "        \n",
    "    \n",
    "    # Find Best Performing model (Maximum R^2 Value)\n",
    "    cv_mse_scores = [one_third_mse, sqrt_mse, n_features_mse]\n",
    "    cv_scores = cv_mse_scores\n",
    "    model_to_use = cv_scores.index(min(cv_scores)) # use model with min MSE\n",
    "    if model_to_use == 0:\n",
    "        rf_model = rf_model_onethird\n",
    "    elif model_to_use == 1:\n",
    "        rf_model = rf_model_sqrt\n",
    "    else: # model_to_use == 2\n",
    "        rf_model = rf_model_n_features\n",
    "    cv_scores_df = pd.DataFrame(cv_scores, index = ['one-third, mse', 'sqrt, mse', 'n_features, mse',], \n",
    "                                columns = [cv_scores_col_name])\n",
    "    save_to_cv_scores = cwd + '/cv_results/' + cv_scores_df_name\n",
    "    cv_scores_df.to_csv(save_to_cv_scores)\n",
    "    \n",
    "    # Plot Results\n",
    "    plt.figure()\n",
    "    plt.bar(x = ['one-third', 'sqrt', 'n_features'], height = cv_mse_scores);\n",
    "    plt.xlabel('Value for max_features', fontsize = 13);\n",
    "    plt.ylabel('Cross Validation MSE Score', fontsize = 13)\n",
    "    plt.title(cv_title)\n",
    "    save_to_bar_chart = cwd + '/figures/' + cv_mse_name\n",
    "    plt.savefig(save_to_bar_chart, dpi = 300, bbox_inches = 'tight');\n",
    "    plt.close()\n",
    "\n",
    "    # Fit Final Model\n",
    "    rf_model.fit(X, y, sample_weight = weights)\n",
    "    # Make predictions for both train and test sets\n",
    "    y_pred = rf_model.predict(X)\n",
    "    # Find Feature Importances\n",
    "    FI = rf_model.feature_importances_\n",
    "    # Evaluation metrics: weighted R^2 and weighted RMSE\n",
    "    r2 = rf_model.score(X, y, sample_weight = weights)\n",
    "    mse = mean_squared_error(y_pred, y, sample_weight = weights)\n",
    "    rmse = np.sqrt(mse)\n",
    "    \n",
    "    # Save Feature Importance Data\n",
    "    FI_df = pd.DataFrame(data = FI, index = X.columns, columns = [FI_col_name])\n",
    "    save_to_FI = cwd + '/feat_imp/' + fi_df_name\n",
    "    FI_df.to_csv(save_to_FI)\n",
    "    \n",
    "    # Save Feature Importance Data as Barchart\n",
    "    primary_col = FI_col_name\n",
    "    primary_col_name = 'normal'\n",
    "    FI_df_sorted = FI_df.sort_values(by = primary_col, ascending = False)\n",
    "    FI_df_sorted[primary_col].plot(kind = 'bar', figsize = (10,5))\n",
    "        \n",
    "    plt.xlabel('Features', fontsize = 15);\n",
    "    plt.ylabel('Feature Importance', fontsize = 15)\n",
    "    plt.title(FI_col_name, fontsize = 15)\n",
    "    save_to_fi_barchart = cwd + '/figures/' + fi_fig_name\n",
    "    plt.savefig(save_to_fi_barchart, dpi = 300, bbox_inches = 'tight');\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Save Evaluation Metrics Data\n",
    "    eval_df = pd.DataFrame(data = [r2, mse, rmse], \n",
    "                           index = ['r2', 'mse', 'rmse'],\n",
    "                                   columns = [eval_col_name])\n",
    "    save_to_eval = cwd + '/eval_metrics/' + eval_df_name\n",
    "    eval_df.to_csv(save_to_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the regression models -- define helpful variables and run the models in a for loop\n",
    "\n",
    "Note 1: The cells that run rf_regression take a *long* time to run.\n",
    "\n",
    "Note 2: These models were run both with and without household density as a feature. If you want to change the presence of household density, modify the creation of the variables in the Data Editing Section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_vals = ['ICL_kWphh_cap', 'ICPVOF_kWphh_cap', 'ICPV_kWphh_cap', 'ICUGOF_kWphh_cap', 'ICUG_kWphh_cap',\n",
    "         'ICPVOF_e_kWphh_cap', 'ICPV_e_kWphh_cap', 'ICUGOF_e_kWphh_cap', 'ICUG_e_kWphh_cap']\n",
    "run_vals = ['all', 'infra', 'service', 'demo']\n",
    "utilities = ['SCE', 'PGE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_regression(y_vals[0], run_vals[3] + '_nohhdensity', X_demo_SCE,\n",
    "              y_demo_SCE[y_vals[0]], 'SCE', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # All Variables\n",
    "    rf_regression(y, run_vals[0], X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', False)\n",
    "    rf_regression(y, run_vals[0], X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', True)\n",
    "    # Demo variables\n",
    "    rf_regression(y, run_vals[3], X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', False)\n",
    "    rf_regression(y, run_vals[3], X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # All Variables\n",
    "    rf_regression(y, run_vals[0], X_all_PGE,\n",
    "                 y_all_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[0], X_all_PGE, \n",
    "                 y_all_PGE[y], 'PGE', True)\n",
    "    # Demo variables\n",
    "    rf_regression(y, run_vals[3], X_demo_PGE,\n",
    "                 y_demo_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[3], X_demo_PGE, \n",
    "                 y_demo_PGE[y], 'PGE', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # All variables\n",
    "    rf_regression(y, run_vals[0] + '_nohhdensity', X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', False)\n",
    "    rf_regression(y, run_vals[0] + '_nohhdensity', X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', True)\n",
    "    # Infrastructure variables\n",
    "    rf_regression(y, run_vals[1], X_infra_SCE,\n",
    "                 y_infra_SCE[y], 'SCE', False)\n",
    "    rf_regression(y, run_vals[1], X_infra_SCE,\n",
    "                 y_infra_SCE[y], 'SCE', True)\n",
    "    # Service variables\n",
    "    rf_regression(y, run_vals[2], X_service_SCE,\n",
    "                 y_service_SCE[y], 'SCE', False)\n",
    "    rf_regression(y, run_vals[2], X_service_SCE,\n",
    "                 y_service_SCE[y], 'SCE', True)\n",
    "    # Demo variables\n",
    "    rf_regression(y, run_vals[3] + '_nohhdensity', X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', False)\n",
    "    rf_regression(y, run_vals[3] + '_nohhdensity', X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # All variables\n",
    "    rf_regression(y, run_vals[0] + '_nohhdensity', X_all_PGE,\n",
    "                 y_all_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[0] + '_nohhdensity', X_all_PGE, \n",
    "                 y_all_PGE[y], 'PGE', True)\n",
    "    # Infrastructure variables\n",
    "    rf_regression(y, run_vals[1], X_infra_PGE, \n",
    "                 y_infra_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[1], X_infra_PGE,\n",
    "                 y_infra_PGE[y], 'PGE', True)\n",
    "    # Service variables\n",
    "    rf_regression(y, run_vals[2], X_service_PGE,\n",
    "                 y_service_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[2], X_service_PGE, \n",
    "                 y_service_PGE[y], 'PGE', True)\n",
    "    # Demo variables\n",
    "    rf_regression(y, run_vals[3] + '_nohhdensity', X_demo_PGE,\n",
    "                 y_demo_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[3] + '_nohhdensity', X_demo_PGE, \n",
    "                 y_demo_PGE[y], 'PGE', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # PG&E\n",
    "    # All variables\n",
    "#     rf_regression(y, run_vals[0], X_train_all_PGE, X_test_all_PGE,\n",
    "#                  y_train_all_PGE[y], y_test_all_PGE[y], 'PGE', False)\n",
    "#     rf_regression(y, run_vals[0], X_train_all_PGE, X_test_all_PGE,\n",
    "#                  y_train_all_PGE[y], y_test_all_PGE[y], 'PGE', True)\n",
    "    # Infrastructure variables\n",
    "    rf_regression(y, run_vals[1], X_train_infra_PGE, X_test_infra_PGE,\n",
    "                 y_train_infra_PGE[y], y_test_infra_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[1], X_train_infra_PGE, X_test_infra_PGE,\n",
    "                 y_train_infra_PGE[y], y_test_infra_PGE[y], 'PGE', True)\n",
    "    # Service variables\n",
    "    rf_regression(y, run_vals[2], X_train_service_PGE, X_test_service_PGE,\n",
    "                 y_train_service_PGE[y], y_test_service_PGE[y], 'PGE', False)\n",
    "    rf_regression(y, run_vals[2], X_train_service_PGE, X_test_service_PGE,\n",
    "                 y_train_service_PGE[y], y_test_service_PGE[y], 'PGE', True)\n",
    "    # Demo variables\n",
    "#     rf_regression(y, run_vals[3], X_train_demo_PGE, X_test_demo_PGE,\n",
    "#                  y_train_demo_PGE[y], y_test_demo_PGE[y], 'PGE', False)\n",
    "#     rf_regression(y, run_vals[3], X_train_demo_PGE, X_test_demo_PGE,\n",
    "#                  y_train_demo_PGE[y], y_test_demo_PGE[y], 'PGE', True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifications <a class=\"anchor\" id=\"bullet4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"toc_1\"></a>\n",
    "* [Data Importing](#bullet1)\n",
    "* [Data Editing](#bullet2)\n",
    "* [Random Forest Regressions](#bullet3)\n",
    "* [Random Forest Classifications](#bullet4)\n",
    "* [Linear Regression Models](#bullet5)\n",
    "* [Logistic Regression Models](#bullet7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions for weighted scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_SCE(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        In multi-label classification, this is the subset accuracy\n",
    "        which is a harsh metric since you require for each sample that\n",
    "        each label set be correctly predicted.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples.\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            True labels for X.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights.\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Mean accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "        weights = SCE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_PGE(self, X, y, sample_weight=None):\n",
    "        \"\"\"\n",
    "        Return the mean accuracy on the given test data and labels.\n",
    "        In multi-label classification, this is the subset accuracy\n",
    "        which is a harsh metric since you require for each sample that\n",
    "        each label set be correctly predicted.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like of shape (n_samples, n_features)\n",
    "            Test samples.\n",
    "        y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
    "            True labels for X.\n",
    "        sample_weight : array-like of shape (n_samples,), default=None\n",
    "            Sample weights.\n",
    "        Returns\n",
    "        -------\n",
    "        score : float\n",
    "            Mean accuracy of self.predict(X) wrt. y.\n",
    "        \"\"\"\n",
    "        weights = PGE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        return accuracy_score(y, self.predict(X), sample_weight=weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for plotting confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(y_true, y_pred, classes, run, max_features,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized Confusion Matrix, ' + run + ' Variables, ' + max_features\n",
    "        else:\n",
    "            title = 'Confusion Matrix, ' + run + ' Variables, ' + max_features\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True Values',\n",
    "           xlabel='Predicted Values');\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\");\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\");\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    # return values used in confusion matrix:\n",
    "    # true negative, false positive, false negative, true positive\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # true positive, normalized\n",
    "    tp_n = tp / (tp + fn)\n",
    "    # false negative, normalized\n",
    "    fn_n = fn / (tp + fn)\n",
    "    # false positive, normalized\n",
    "    fp_n = fp / (tn + fp)\n",
    "    # true negative, normalized\n",
    "    tn_n = tn / (tn + fp)\n",
    "    cm_vals = [tp, fn, fp, tn, tp_n, fn_n, fp_n, tn_n]\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    \n",
    "    return ax, cm_vals, precision, recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classification Model Function without train-test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rf_classification(y_val, run, X, y, utility, cutoff):\n",
    "    \"\"\"\n",
    "    Run the Random Forest Classification Models\n",
    "    Find the best value for max_features using cross validation\n",
    "    Save figure comparing CV Scores\n",
    "    Save CSVs of feature importances and errors\n",
    "    Create and save error plots\n",
    "    Create and save confusion matrices \n",
    "    \n",
    "    y options:\n",
    "    0. ICL_kWphh\n",
    "    1. ICPVOF_kWphh\n",
    "    2. ICPV_kWphh\n",
    "    3. ICUGOF_kWphh\n",
    "    4. ICUG_kWphh\n",
    "    \n",
    "    Run options:\n",
    "    0. all variables\n",
    "    1. infra variables\n",
    "    2. service variables\n",
    "    3. demo variables\n",
    "    \n",
    "    Cutoff should be an int or float\n",
    "    \"\"\"\n",
    "    \n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    # y_train and y_test should be the same as if this was a regression run\n",
    "    # change these to boolean values using the supplied cutoff value\n",
    "    y = y >= cutoff\n",
    "    trues = sum(y)\n",
    "    falses = len(y) - trues\n",
    "    \n",
    "#     if '_cap' in y:\n",
    "#         y_mod = y.replace('_cap', '')\n",
    "    \n",
    "    # Determine best model for max_features\n",
    "    rf_model_onethird = RandomForestClassifier(max_features = 1/3)\n",
    "    rf_model_sqrt = RandomForestClassifier(max_features = 'sqrt')\n",
    "    rf_model_n_features = RandomForestClassifier()\n",
    "    \n",
    "    # Perform Cross Validation (depends on utility)\n",
    "    # Also define other variables that are utility-dependent (ex. when saving CSVs)\n",
    "    if utility == 'SCE':\n",
    "        weights = SCE_realdata_encoded.loc[X.index, 'tothh_Cpoly']\n",
    "#         test_weights = SCE_realdata_encoded.loc[X_test.index, 'tothh_Cpoly']\n",
    "        one_third_score = np.mean(cross_val_score(rf_model_onethird, X, y, \n",
    "                                              fit_params = {'sample_weight' : weights},\n",
    "                                              cv = 10, scoring = score_SCE))\n",
    "        sqrt_score = np.mean(cross_val_score(rf_model_sqrt, X, y, \n",
    "                                         fit_params = {'sample_weight' : weights},\n",
    "                                         cv = 10, scoring = score_SCE))\n",
    "        n_features_score = np.mean(cross_val_score(rf_model_n_features, X, y, \n",
    "                                               fit_params = {'sample_weight' : weights},\n",
    "                                               cv = 10, scoring = score_SCE))\n",
    "        cv_scores_col_name = 'SCE Classification CV Scores for ' + y_val + ' with ' + run + ' Variables'\n",
    "        cv_scores_df_name = 'SCE_C_cv_' + y_val + '_' + run + '.csv'\n",
    "        FI_col_name = 'SCE Classification FI for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        fi_df_name = 'SCE_C_FeatImp_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "        fi_fig_name = 'SCE_C_FI_' + y_val + '_' + run + '_' + str(cutoff) + '.png'\n",
    "        eval_col_name = 'SCE Classification Eval metrics for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        eval_df_name = 'SCE_C_Eval_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "        fig_title = 'SCE Classification Cross Validation Scores for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        fig_name = 'SCE_C_cv_' + y_val + '_' + run + '_' + str(cutoff) + '.png'\n",
    "        cm_title = 'SCE Confusion Matrix for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        cm_fig = 'SCE_C_cm_' + y_val + run + '_' + str(cutoff) + '.png'\n",
    "        cm_fig_norm = 'SCE_C_cm_' + y_val + run + '_norm_' + str(cutoff) + '.png'\n",
    "    \n",
    "    else: # utility == 'PGE'\n",
    "        weights = PGE_realdata_encoded.loc[X.index, 'tothh_Cpoly']\n",
    "#         test_weights = PGE_realdata_encoded.loc[X.index, 'tothh_Cpoly']\n",
    "        one_third_score = np.mean(cross_val_score(rf_model_onethird, X, y, \n",
    "                                              fit_params = {'sample_weight' : weights},\n",
    "                                              cv = 10, scoring = score_PGE))\n",
    "        sqrt_score = np.mean(cross_val_score(rf_model_sqrt, X, y, \n",
    "                                         fit_params = {'sample_weight' : weights},\n",
    "                                         cv = 10, scoring = score_PGE))\n",
    "        n_features_score = np.mean(cross_val_score(rf_model_n_features, X, y, \n",
    "                                               fit_params = {'sample_weight' : weights},\n",
    "                                               cv = 10, scoring = score_PGE))\n",
    "        cv_scores_col_name = 'PG&E Classification CV Scores for ' + y_val + ' with ' + run + ' Variables'\n",
    "        cv_scores_df_name = 'PGE_C_cv_' + y_val + '_' + run + '.csv'\n",
    "        FI_col_name = 'PG&E Classification FI for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        fi_df_name = 'PGE_C_FeatImp_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "        fi_fig_name = 'PGE_C_FI_' + y_val + '_' + run + '_' + str(cutoff) + '.png'\n",
    "        eval_col_name = 'PG&E Classification Eval metrics for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        eval_df_name = 'PGE_C_Eval_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "        fig_title = 'PG&E Classification Cross Validation Scores for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        fig_name = 'PGE_C_cv_' + y_val + '_' + run + '_' + str(cutoff) + '.png'\n",
    "        cm_title = 'PG&E Confusion Matrix for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        cm_fig = 'PGE_C_cm_' + y_val + run + '_' + str(cutoff) + '.png'\n",
    "        cm_fig_norm = 'PGE_C_cm_' + y_val + run + '_norm_' + str(cutoff) + '.png'\n",
    "        \n",
    "    \n",
    "    # Find Best Performing model (Maximum R^2 Value)\n",
    "    cv_scores = [one_third_score, sqrt_score, n_features_score]\n",
    "    model_to_use = cv_scores.index(max(cv_scores))\n",
    "    if model_to_use == 0:\n",
    "        rf_model = rf_model_onethird\n",
    "        max_features = '1/3'\n",
    "    elif model_to_use == 1:\n",
    "        rf_model = rf_model_sqrt\n",
    "        max_features = 'sqrt'\n",
    "    else: # model_to_use == 2\n",
    "        rf_model = rf_model_n_features\n",
    "        max_features = 'n_features'\n",
    "    cv_scores_df = pd.DataFrame(cv_scores, index = ['one-third', 'sqrt', 'n_features'], \n",
    "                                columns = [cv_scores_col_name])\n",
    "    save_to_cv_scores = cwd + '/cv_results/' + cv_scores_df_name\n",
    "    cv_scores_df.to_csv(save_to_cv_scores)\n",
    "    \n",
    "    # Plot Results     \n",
    "    plt.figure()\n",
    "    plt.bar(x = ['one-third', 'sqrt', 'n_features'], height = cv_scores);\n",
    "    plt.xlabel('Value for max_features', fontsize = 13);\n",
    "    plt.ylabel('Cross Validation Score', fontsize = 13)\n",
    "    plt.title(fig_title)\n",
    "    plt.tight_layout()\n",
    "    save_to_bar_chart = cwd + '/figures/' + fig_name\n",
    "    plt.savefig(save_to_bar_chart, dpi = 300, bbox_inches = 'tight');\n",
    "    plt.close()\n",
    "\n",
    "    # Fit Final Model\n",
    "    rf_model.fit(X, y, sample_weight = weights)\n",
    "    # Make predictions for both train and test sets\n",
    "    y_pred = rf_model.predict(X)\n",
    "    # Find Feature Importances\n",
    "    FI = rf_model.feature_importances_\n",
    "    # Evaluation metrics: weighted scores\n",
    "    score = rf_model.score(X, y, sample_weight = weights)\n",
    "    \n",
    "    # Save Feature Importance Data\n",
    "    FI_df = pd.DataFrame(data = FI, index = X.columns, columns = [FI_col_name])\n",
    "    save_to_FI = cwd + '/feat_imp/' + fi_df_name\n",
    "    FI_df.to_csv(save_to_FI)\n",
    "    \n",
    "    # Save Feature Importance Data as Barchart\n",
    "    FI_df_sorted = FI_df.sort_values(by = FI_col_name, ascending = False)\n",
    "    FI_df_sorted.plot(kind = 'bar', figsize = (10,5))\n",
    "    plt.xlabel('Features', fontsize = 15);\n",
    "    plt.ylabel('Feature Importance', fontsize = 15)\n",
    "    plt.title(FI_col_name, fontsize = 15)\n",
    "    save_to_fi_barchart = cwd + '/figures/' + fi_fig_name\n",
    "    plt.savefig(save_to_fi_barchart, dpi = 300, bbox_inches = 'tight');\n",
    "    plt.close()\n",
    "    \n",
    "    \n",
    "    # Confusion Matrices\n",
    "    axis_vals = ['<' + str(cutoff) + ' kW/hh', '>=' + str(cutoff) + ' kW/hh']\n",
    "    # Regular Confusion Matrix (train)\n",
    "    ax, cm_vals, precision, recall = plot_confusion_matrix(y, y_pred,\n",
    "                     axis_vals, run, max_features, title = cm_title);\n",
    "    save_to_cm1 = cwd + '/figures/' + cm_fig\n",
    "    plt.savefig(save_to_cm1, dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()\n",
    "    # Normalized Confusion Matrix (train)\n",
    "    plot_confusion_matrix(y, y_pred,\n",
    "                     axis_vals, run, max_features, normalize = True, title = cm_title);\n",
    "    save_to_cm2 = cwd + '/figures/' + cm_fig_norm\n",
    "    plt.savefig(save_to_cm2, dpi = 300, bbox_inches = 'tight')\n",
    "    plt.close()\n",
    "    \n",
    "    # Save Evaluation Metrics Data\n",
    "    eval_df = pd.DataFrame(data = [score, \n",
    "                                   precision, recall, \n",
    "                                   cm_vals[0], cm_vals[1], cm_vals[2], cm_vals[3],\n",
    "                                   cm_vals[4], cm_vals[5], cm_vals[6], cm_vals[7],\n",
    "                                   trues, falses], \n",
    "                           index = ['score', \n",
    "                                    'precision', 'recall',\n",
    "                                    'true_positive', 'false_negative', 'false_positive', 'true_negative',\n",
    "                                    'true_positive_norm', 'false_negative_norm', 'false_positive_norm', 'true_negative_norm',\n",
    "                                    'num_trues', 'num_falses'],\n",
    "                                   columns = [eval_col_name])\n",
    "    save_to_eval = cwd + '/eval_metrics/' + eval_df_name\n",
    "    eval_df.to_csv(save_to_eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the classification models. Note the following cells take a *long* time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # Demo variables\n",
    "    rf_classification(y, run_vals[0], X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', 10)\n",
    "    rf_classification(y, run_vals[0], X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', 1.5)\n",
    "    rf_classification(y, run_vals[3], X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', 10)\n",
    "    rf_classification(y, run_vals[3], X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', 1.5)\n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # Demo variables\n",
    "    rf_classification(y, run_vals[0], X_all_PGE,\n",
    "                 y_all_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[0], X_all_PGE,\n",
    "                 y_all_PGE[y], 'PGE', 1.5)\n",
    "    rf_classification(y, run_vals[3], X_demo_PGE, \n",
    "                 y_demo_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[3], X_demo_PGE,\n",
    "                 y_demo_PGE[y], 'PGE', 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # All variables\n",
    "    rf_classification(y, run_vals[0] + '_nohhdensity', X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', 10)\n",
    "    rf_classification(y, run_vals[0] + '_nohhdensity', X_all_SCE,\n",
    "                 y_all_SCE[y], 'SCE', 1.5)\n",
    "    # Infrastructure variables\n",
    "    rf_classification(y, run_vals[1], X_infra_SCE,\n",
    "                 y_infra_SCE[y], 'SCE', 10)\n",
    "    rf_classification(y, run_vals[1], X_infra_SCE,\n",
    "                 y_infra_SCE[y], 'SCE', 1.5)\n",
    "    # Service variables\n",
    "    rf_classification(y, run_vals[2], X_service_SCE,\n",
    "                 y_service_SCE[y], 'SCE', 10)\n",
    "    rf_classification(y, run_vals[2], X_service_SCE,\n",
    "                 y_service_SCE[y], 'SCE', 1.5)\n",
    "    # Demo variables\n",
    "    rf_classification(y, run_vals[3] + '_nohhdensity', X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', 10)\n",
    "    rf_classification(y, run_vals[3] + '_nohhdensity', X_demo_SCE,\n",
    "                 y_demo_SCE[y], 'SCE', 1.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # All variables\n",
    "    rf_classification(y, run_vals[0] + '_nohhdensity', X_all_PGE,\n",
    "                 y_all_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[0] + '_nohhdensity', X_all_PGE,\n",
    "                 y_all_PGE[y], 'PGE', 1.5)\n",
    "    # Infrastructure variables\n",
    "    rf_classification(y, run_vals[1], X_infra_PGE,\n",
    "                 y_infra_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[1], X_infra_PGE, \n",
    "                 y_infra_PGE[y], 'PGE', 1.5)\n",
    "    # Service variables\n",
    "    rf_classification(y, run_vals[2], X_service_PGE,\n",
    "                 y_service_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[2], X_service_PGE,\n",
    "                 y_service_PGE[y], 'PGE', 1.5)\n",
    "    # Demo variables\n",
    "    rf_classification(y, run_vals[3] + '_nohhdensity', X_demo_PGE, \n",
    "                 y_demo_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[3] + '_nohhdensity', X_demo_PGE,\n",
    "                 y_demo_PGE[y], 'PGE', 1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # PG&E\n",
    "    # All variables\n",
    "#     rf_classification(y, run_vals[0], X_train_all_PGE, X_test_all_PGE,\n",
    "#                  y_train_all_PGE[y], y_test_all_PGE[y], 'PGE', 10)\n",
    "#     rf_classification(y, run_vals[0], X_train_all_PGE, X_test_all_PGE,\n",
    "#                  y_train_all_PGE[y], y_test_all_PGE[y], 'PGE', 1.5)\n",
    "    # Infrastructure variables\n",
    "    rf_classification(y, run_vals[1], X_train_infra_PGE, X_test_infra_PGE,\n",
    "                 y_train_infra_PGE[y], y_test_infra_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[1], X_train_infra_PGE, X_test_infra_PGE,\n",
    "                 y_train_infra_PGE[y], y_test_infra_PGE[y], 'PGE', 1.5)\n",
    "    # Service variables\n",
    "    rf_classification(y, run_vals[2], X_train_service_PGE, X_test_service_PGE,\n",
    "                 y_train_service_PGE[y], y_test_service_PGE[y], 'PGE', 10)\n",
    "    rf_classification(y, run_vals[2], X_train_service_PGE, X_test_service_PGE,\n",
    "                 y_train_service_PGE[y], y_test_service_PGE[y], 'PGE', 1.5)\n",
    "    # Demo variables\n",
    "#     rf_classification(y, run_vals[3], X_train_demo_PGE, X_test_demo_PGE,\n",
    "#                  y_train_demo_PGE[y], y_test_demo_PGE[y], 'PGE', 10)\n",
    "#     rf_classification(y, run_vals[3], X_train_demo_PGE, X_test_demo_PGE,\n",
    "#                  y_train_demo_PGE[y], y_test_demo_PGE[y], 'PGE', 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression Models <a class=\"anchor\" id=\"bullet5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"toc_1\"></a>\n",
    "* [Data Importing](#bullet1)\n",
    "* [Data Editing](#bullet2)\n",
    "* [Random Forest Regressions](#bullet3)\n",
    "* [Random Forest Classifications](#bullet4)\n",
    "* [Linear Regression Models](#bullet5)\n",
    "* [Logistic Regression Models](#bullet7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression function that does not rely on train-test split. If you would like to utilize a train-test split, you can remove some of the comments from the following function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linreg(y_val, run, X, y, utility):\n",
    "    \"\"\"\n",
    "    Run Linear Regression Methods on a Independent & Dependent Variable Set\n",
    "    CI_val should be an int, ex. 90, 95\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "#     if '_cap' in y:\n",
    "#         y_mod = y.replace('_cap', '')\n",
    "    \n",
    "    # Normalize\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    \n",
    "    if utility == 'SCE':\n",
    "        sample_weights = SCE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        sample_weights_all = SCE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        CI_df_name = 'SCE_CI_' + y_val + '_' + run + '.csv'\n",
    "        coef_df_name = 'SCE_LinMod_coef_' + y_val + '_' + run + '.csv'\n",
    "        eval_col_name = 'SCE Eval metrics for ' + y_val + ' with ' + run + ' Variables'\n",
    "        eval_df_name = 'SCE_LinMod_Eval_' + y_val + '_' + run + '.csv'\n",
    "    else: # utility == 'PGE'\n",
    "        sample_weights = PGE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        sample_weights_all = PGE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        CI_df_name = 'PGE_CI_' + y_val + '_' + run + '.csv'\n",
    "        coef_df_name = 'PGE_LinMod_coef_' + y_val + '_' + run + '.csv'\n",
    "        eval_col_name = 'PG&E Eval metrics for ' + y_val + ' with ' + run + ' Variables'\n",
    "        eval_df_name = 'PGE_LinMod_Eval_' + y_val + '_' + run + '.csv'\n",
    "    \n",
    "    # Model Creation and Evalution\n",
    "#     lm = LinearRegression().fit(X_train, y_train, sample_weight = sample_weights)\n",
    "    lm_all = LinearRegression().fit(X, y, sample_weight = sample_weights_all)\n",
    "    if utility == 'SCE':\n",
    "#         train_mse = mean_squared_error_weighted_SCE(lm, X_train, y_train)\n",
    "#         train_rmse = np.sqrt(train_mse)\n",
    "#         test_mse = np.sqrt(mean_squared_error_weighted_SCE(lm, X_test, y_test))\n",
    "#         test_rmse = np.sqrt(test_mse)\n",
    "        all_mse = mean_squared_error_weighted_SCE(lm_all, X, y)\n",
    "        all_rmse = np.sqrt(all_mse)\n",
    "#         r2_test = r2_score_weighted_SCE(lm, X_test, y_test)\n",
    "#         r2_train = r2_score_weighted_SCE(lm, X_train, y_train)\n",
    "        r2_all = r2_score_weighted_SCE(lm_all, X, y)\n",
    "    else: # utility = 'PGE'\n",
    "#         train_mse = mean_squared_error_weighted_PGE(lm, X_train, y_train)\n",
    "#         train_rmse = np.sqrt(train_mse)\n",
    "#         test_mse = mean_squared_error_weighted_PGE(lm, X_test, y_test)\n",
    "#         test_rmse = np.sqrt(test_mse)\n",
    "        all_mse = mean_squared_error_weighted_PGE(lm_all, X, y)\n",
    "        all_rmse = np.sqrt(all_mse)\n",
    "#         r2_test = r2_score_weighted_PGE(lm, X_test, y_test)\n",
    "#         r2_train = r2_score_weighted_PGE(lm, X_train, y_train)\n",
    "        r2_all = r2_score_weighted_PGE(lm_all, X, y)\n",
    "    \n",
    "#     eval_metrics = pd.DataFrame(data = [r2_train, r2_test, r2_all, \n",
    "#                                         train_mse, test_mse, all_mse, \n",
    "#                                         train_rmse, test_rmse, all_rmse],\n",
    "#                                index = ['r2_train', 'r2_test', 'r2_all', \n",
    "#                                         'mse_train', 'mse_test', 'mse_all', \n",
    "#                                         'rmse_train', 'rmse_test', 'rmse_all'],\n",
    "#                                columns = [eval_col_name])\n",
    "\n",
    "    eval_metrics = pd.DataFrame(data = [r2_all, \n",
    "                                        all_mse, \n",
    "                                        all_rmse],\n",
    "                               index = ['r2_all', \n",
    "                                        'mse_all', \n",
    "                                        'rmse_all'],\n",
    "                               columns = [eval_col_name])\n",
    "    coef_df = pd.DataFrame(data = lm_all.coef_, index = X.columns, columns=['Linear Model Coefs'])\n",
    "#     coef_df['No Split'] = lm_all.coef_\n",
    "#     coef_df['Split - No Split'] = coef_df['Train/Test Split'] - coef_df['No Split']\n",
    "    \n",
    "    \n",
    "    # Find Standard Errors Using Linear Algebra\n",
    "#     y_hat_train = lm.predict(X_train)\n",
    "#     residuals_train = y_train - y_hat_train\n",
    "#     residenal_sum_of_squares_train = residuals_train.T @ residuals_train\n",
    "#     sigma_squared_hat_train = residenal_sum_of_squares_train / (X_train.shape[0] - (X_train.shape[1]))\n",
    "#     var_beta_hat_train = np.linalg.inv(X_train.T @ X_train) * sigma_squared_hat_train\n",
    "#     standard_errors_train = []\n",
    "#     for p_ in range(X_train.shape[1]):\n",
    "#         standard_errors_train.append(var_beta_hat_train[p_, p_] ** 0.5)\n",
    "        \n",
    "    y_hat_all = lm_all.predict(X)\n",
    "    residuals_all = y - y_hat_all\n",
    "    residenal_sum_of_squares_all = residuals_all.T @ residuals_all\n",
    "    sigma_squared_hat_all = residenal_sum_of_squares_all / (X.shape[0] - (X.shape[1]))\n",
    "    var_beta_hat_all = np.linalg.inv(X.T @ X) * sigma_squared_hat_all\n",
    "    standard_errors_all = []\n",
    "    for p_ in range(X.shape[1]):\n",
    "        standard_errors_all.append(var_beta_hat_all[p_, p_] ** 0.5)\n",
    "        \n",
    "#     coef_df['Split Standard Errors'] = standard_errors_train\n",
    "    coef_df['Standard Errors'] = standard_errors_all\n",
    "#     coef_df['Split - No Split Standard Error'] = np.array(standard_errors_train) - np.array(standard_errors_all)\n",
    "#     coef_df['Split - No Split Standard Error (% Change)'] = (np.array(standard_errors_train) - np.array(standard_errors_all))/coef_df['Split Standard Errors']*100\n",
    "    \n",
    "    # Save Eval Metrics and Coefficients\n",
    "    save_to_eval = cwd + '/lm_eval_metrics/' + eval_df_name\n",
    "    eval_metrics.to_csv(save_to_eval)\n",
    "    save_to_coef = cwd + '/lm_coef/' + coef_df_name\n",
    "    coef_df.to_csv(save_to_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove collinear columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_SCE_mod = X_all_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_all_PGE_mod = X_all_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_SCE_mod = X_service_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_PGE_mod = X_service_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the linear regressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # All variables\n",
    "    linreg(y, run_vals[0] + '_nohhdensity', X_all_SCE_mod, y_all_SCE[y], 'SCE')\n",
    "    # Infrastructure variables\n",
    "    linreg(y, run_vals[1], X_infra_SCE, y_infra_SCE[y], 'SCE')\n",
    "    # Service variables\n",
    "    linreg(y, run_vals[2], X_service_SCE_mod, y_service_SCE[y], 'SCE')\n",
    "    # Demo variables\n",
    "    linreg(y, run_vals[3] + '_nohhdensity', X_demo_SCE, y_demo_SCE[y], 'SCE')\n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # All variables\n",
    "    linreg(y, run_vals[0] + '_nohhdensity', X_all_PGE_mod, y_all_PGE[y], 'PGE')\n",
    "    # Infrastructure variables\n",
    "    linreg(y, run_vals[1], X_infra_PGE, y_infra_PGE[y], 'PGE')\n",
    "    # Service variables\n",
    "    linreg(y, run_vals[2], X_service_PGE_mod, y_service_PGE[y],'PGE')\n",
    "    # Demo variables\n",
    "    linreg(y, run_vals[3] + '_nohhdensity', X_demo_PGE, y_demo_PGE[y], 'PGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_SCE_mod = X_all_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_all_PGE_mod = X_all_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_SCE_mod = X_service_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_PGE_mod = X_service_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # All variables\n",
    "    linreg(y, run_vals[0], X_all_SCE_mod, y_all_SCE[y], 'SCE')\n",
    "    # Demo variables\n",
    "    linreg(y, run_vals[3], X_demo_SCE, y_demo_SCE[y], 'SCE')\n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # All variables\n",
    "    linreg(y, run_vals[0], X_all_PGE_mod, y_all_PGE[y], 'PGE')\n",
    "    # Demo variables\n",
    "    linreg(y, run_vals[3], X_demo_PGE, y_demo_PGE[y], 'PGE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in y_vals:\n",
    "    # PG&E\n",
    "    # All variables\n",
    "#     linreg(y, run_vals[0], X_train_all_PGE, X_test_all_PGE,\n",
    "#            y_train_all_PGE[y], y_test_all_PGE[y],\n",
    "#            X_all_PGE, y_all_PGE[y], 'PGE')\n",
    "    # Infrastructure variables\n",
    "    linreg(y, run_vals[1], X_train_infra_PGE, X_test_infra_PGE,\n",
    "           y_train_infra_PGE[y], y_test_infra_PGE[y],\n",
    "           X_infra_PGE, y_infra_PGE[y], 'PGE')\n",
    "    # Service variables\n",
    "    linreg(y, run_vals[2], X_train_service_PGE, X_test_service_PGE,\n",
    "           y_train_service_PGE[y], y_test_service_PGE[y], \n",
    "           X_service_PGE, y_service_PGE[y],'PGE')\n",
    "    # Demo variables\n",
    "#     linreg(y, run_vals[3], X_train_demo_PGE, X_test_demo_PGE,\n",
    "#            y_train_demo_PGE[y], y_test_demo_PGE[y], \n",
    "#            X_demo_PGE, y_demo_PGE[y], 'PGE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regressions<a class=\"anchor\" id=\"bullet7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"toc_1\"></a>\n",
    "* [Data Importing](#bullet1)\n",
    "* [Data Editing](#bullet2)\n",
    "* [Random Forest Regressions](#bullet3)\n",
    "* [Random Forest Classifications](#bullet4)\n",
    "* [Linear Regression Models](#bullet5)\n",
    "* [Logistic Regression Models](#bullet7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function for Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(y_val, run, X, y, utility, cutoff):\n",
    "    \"\"\"\n",
    "    Run Linear Regression Methods on a Independent & Dependent Variable Set\n",
    "    CI_val should be an int, ex. 90, 95\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    \n",
    "    y = y >= cutoff\n",
    "    trues = sum(y)\n",
    "    falses = len(y) - trues\n",
    "    \n",
    "    # Normalize\n",
    "    X = (X - X.mean()) / X.std()\n",
    "    \n",
    "    if utility == 'SCE':\n",
    "        sample_weights_all = SCE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        coef_df_name = 'SCE_LogMod_coef_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "        eval_col_name = 'SCE Eval metrics for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        eval_df_name = 'SCE_LogMod_Eval_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "    else: # utility == 'PGE'\n",
    "        sample_weights_all = PGE_realdata_encoded.loc[X.index.tolist(),'tothh_Cpoly']\n",
    "        coef_df_name = 'PGE_LogMod_coef_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "        eval_col_name = 'PG&E Eval metrics for ' + y_val + ' with ' + run + ' Variables, ' + str(cutoff) + ' kW/hh Cutoff'\n",
    "        eval_df_name = 'PGE_LogMod_Eval_' + y_val + '_' + run + '_' + str(cutoff) + '.csv'\n",
    "    \n",
    "    # Model Creation and Evalution\n",
    "    lm_all = LogisticRegression().fit(X, y, sample_weight = sample_weights_all)\n",
    "    score = lm_all.score(X, y, sample_weight = sample_weights_all)\n",
    "        \n",
    "    y_hat_all = lm_all.predict(X)\n",
    "#     residuals_all = y - y_hat_all\n",
    "#     residenal_sum_of_squares_all = residuals_all.T @ residuals_all\n",
    "#     sigma_squared_hat_all = residenal_sum_of_squares_all / (X.shape[0] - (X.shape[1]))\n",
    "#     var_beta_hat_all = np.linalg.inv(X.T @ X) * sigma_squared_hat_all\n",
    "#     standard_errors_all = []\n",
    "#     for p_ in range(X.shape[1]):\n",
    "#         standard_errors_all.append(var_beta_hat_all[p_, p_] ** 0.5)\n",
    "\n",
    "    # Design matrix -- add column of 1's at the beginning of your X_train matrix\n",
    "    X_design = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "    # Initiate matrix of 0's, fill diagonal with each predicted observation's variance\n",
    "    V = np.diagflat(np.product(lm_all.predict_proba(X), axis=1))\n",
    "\n",
    "    # Covariance matrix\n",
    "    covLogit = np.linalg.inv(X_design.T @ V @ X_design)\n",
    "    standard_errors_all = []\n",
    "    for p_ in range(X.shape[1]):\n",
    "        standard_errors_all.append(covLogit[p_, p_] ** 0.5)\n",
    "#     print(\"Covariance matrix: \", covLogit)\n",
    "\n",
    "#     # Standard errors\n",
    "#     print(\"Standard errors: \", np.sqrt(np.diag(covLogit)))\n",
    "        \n",
    "    cm = confusion_matrix(y, y_hat_all)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    # true positive, normalized\n",
    "    tp_n = tp / (tp + fn)\n",
    "    # false negative, normalized\n",
    "    fn_n = fn / (tp + fn)\n",
    "    # false positive, normalized\n",
    "    fp_n = fp / (tn + fp)\n",
    "    # true negative, normalized\n",
    "    tn_n = tn / (tn + fp)\n",
    "    cm_vals = [tp, fn, fp, tn, tp_n, fn_n, fp_n, tn_n]\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    if recall == 0:\n",
    "        print(precision)\n",
    "        print(recall)\n",
    "        print('true positive:\\t' + str(tp))\n",
    "        print('false negative:\\t' + str(fn))\n",
    "        print('false positive:\\t' + str(fp))\n",
    "        print('true negative:\\t' + str(tn))\n",
    "        print()\n",
    "\n",
    "    eval_metrics = pd.DataFrame(data = [score, precision, recall, \n",
    "                                       tp, fn, fp, tn,\n",
    "                                       tp_n, fn_n, fp_n, tn_n,\n",
    "                                       trues, falses],\n",
    "                               index = ['score', \n",
    "                                    'precision', 'recall',\n",
    "                                    'true_positive', 'false_negative', 'false_positive', 'true_negative',\n",
    "                                    'true_positive_norm', 'false_negative_norm', 'false_positive_norm', 'true_negative_norm',\n",
    "                                    'num_trues', 'num_falses'],\n",
    "                                   columns = [eval_col_name])\n",
    "    \n",
    "    \n",
    "    coef_df = pd.DataFrame(data = lm_all.coef_, index = ['Logistic Model Coefs'], columns=X.columns) \n",
    "    coef_df = coef_df.T\n",
    "    coef_df['Standard Errors'] = standard_errors_all\n",
    "    \n",
    "    # Save Eval Metrics and Coefficients\n",
    "    save_to_eval = cwd + '/log_eval_metrics/' + eval_df_name\n",
    "    eval_metrics.to_csv(save_to_eval)\n",
    "    save_to_coef = cwd + '/log_coef/' + coef_df_name\n",
    "    coef_df.to_csv(save_to_coef)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop collinear variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_SCE_mod = X_all_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_all_PGE_mod = X_all_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_SCE_mod = X_service_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_PGE_mod = X_service_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennyconde/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t2616\n",
      "false positive:\t0\n",
      "true negative:\t18607\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t919\n",
      "false positive:\t0\n",
      "true negative:\t28986\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t474\n",
      "false positive:\t0\n",
      "true negative:\t29431\n",
      "\n",
      "0.0\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t204\n",
      "false positive:\t1\n",
      "true negative:\t22319\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t192\n",
      "false positive:\t0\n",
      "true negative:\t21031\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t592\n",
      "false positive:\t0\n",
      "true negative:\t20631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # All variables\n",
    "    logreg(y, run_vals[0], X_all_SCE_mod,\n",
    "           y_all_SCE[y], 'SCE', 10)\n",
    "    logreg(y, run_vals[0], X_all_SCE_mod,\n",
    "                 y_all_SCE[y], 'SCE', 1.5)\n",
    "    # Infrastructure variables\n",
    "    logreg(y, run_vals[1], X_infra_SCE,\n",
    "           y_infra_SCE[y], 'SCE', 10)\n",
    "    logreg(y, run_vals[1], X_infra_SCE,\n",
    "           y_infra_SCE[y], 'SCE', 1.5)\n",
    "    # Service variables\n",
    "    logreg(y, run_vals[2], X_service_SCE_mod,\n",
    "           y_service_SCE[y], 'SCE', 10)\n",
    "    logreg(y, run_vals[2], X_service_SCE_mod,\n",
    "                 y_service_SCE[y], 'SCE', 1.5)\n",
    "    # Demo variables\n",
    "    logreg(y, run_vals[3], X_demo_SCE,\n",
    "           y_demo_SCE[y], 'SCE', 10)\n",
    "    logreg(y, run_vals[3], X_demo_SCE,\n",
    "           y_demo_SCE[y], 'SCE', 1.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # All variables\n",
    "    logreg(y, run_vals[0], X_all_PGE_mod,\n",
    "           y_all_PGE[y], 'PGE', 10)\n",
    "    logreg(y, run_vals[0], X_all_PGE_mod, \n",
    "                 y_all_PGE[y], 'PGE', 1.5)\n",
    "    # Infrastructure variables\n",
    "    logreg(y, run_vals[1], X_infra_PGE, \n",
    "                 y_infra_PGE[y], 'PGE', 10)\n",
    "    logreg(y, run_vals[1], X_infra_PGE,\n",
    "                 y_infra_PGE[y], 'PGE', 1.5)\n",
    "    # Service variables\n",
    "    logreg(y, run_vals[2], X_service_PGE_mod,\n",
    "                 y_service_PGE[y], 'PGE', 10)\n",
    "    logreg(y, run_vals[2], X_service_PGE_mod, \n",
    "                 y_service_PGE[y], 'PGE', 1.5)\n",
    "    # Demo variables\n",
    "    logreg(y, run_vals[3], X_demo_PGE,\n",
    "                 y_demo_PGE[y], 'PGE', 10)\n",
    "    logreg(y, run_vals[3], X_demo_PGE, \n",
    "                 y_demo_PGE[y], 'PGE', 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Redefined the variables here for convenience since this Python notebook is very long. Makes it more accessible to add/remove household density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SCE Independent Variable Sets\n",
    "\n",
    "X_SCE_infra_cols = ['CircVolt_kV', 'Phase_max', 'Phase_min', 'CLSmax', 'CLSmin', \n",
    "                    'Length_m', 'Length_m_ctot']\n",
    "# 'TotalDG',\n",
    "# SCE_realdata_encoded.columns[10:25].tolist() + SCE_realdata_encoded.columns[54:57].tolist()\n",
    "\n",
    "X_SCE_service_cols = ['Res_pct', 'Com_pct', 'Ind_pct', 'Agr_pct', 'Oth_pct', 'tothh_Cpoly',\n",
    "                        'tothh_ctot', 'tothh_pct', 'tothh_perkm', 'ResCust', 'urbanheat_pctl', \n",
    "                        'ghi_kWhpm2day', 'SAIDI5yravg']\n",
    "# SCE_realdata_encoded.columns[25:35].tolist()\n",
    "\n",
    "X_SCE_demo_cols = ['tothh', 'racediversity', 'black_pct', 'asian_pct', 'nlxwhite_pct', \n",
    "                   'latinx_pct', 'inc50kbelow_pct', 'inc150kplus_pct', 'medhhinc', 'edavgyrs',\n",
    "                   'ownerocc_pct', 'singleunit_pct', 'unitsavg', 'medyrbuilt',\n",
    "                   'polexposure_pctl', 'polenvt_pctl', 'popsens_pctl',\n",
    "                   'lingisolation_pctl', 'sb535disad_Yes']\n",
    "# 'hhdensity_hhsqkm',\n",
    "#  'hhdensity_fctr_rural', 'hhdensity_fctr_suburban', 'hhdensity_fctr_urban'\n",
    "# SCE_realdata_encoded.columns[35:54].tolist() + SCE_realdata_encoded.columns[57:58].tolist()\n",
    "\n",
    "# X_SCE_infrademo_demo_cols = X_SCE_infrademo_cols + X_SCE_demo_cols\n",
    "X_SCE_all_cols = X_SCE_infra_cols + X_SCE_service_cols + X_SCE_demo_cols\n",
    "\n",
    "\n",
    "# PG&E Independent Variable Sets\n",
    "\n",
    "X_PGE_infra_cols = ['CircVolt_kV', 'Length_m', 'ICA_pct', \n",
    "                    'Length_m_ctot']\n",
    "# 'TotalDG',\n",
    "# PGE_realdata_encoded.columns[10:18].tolist()\n",
    "\n",
    "X_PGE_service_cols = ['Res_pct', 'Com_pct', 'Ind_pct', 'Agr_pct', 'Oth_pct', 'tothh_Cpoly', \n",
    "                        'tothh_ctot', 'tothh_pct', 'tothh_perkm', 'ResCust', 'urbanheat_pctl', \n",
    "                        'ghi_kWhpm2day', 'SAIDI5yravg']\n",
    "# PGE_realdata_encoded.columns[18:28].tolist()\n",
    "\n",
    "X_PGE_demo_cols = ['tothh', 'racediversity', 'black_pct', 'asian_pct', 'nlxwhite_pct', \n",
    "                   'latinx_pct', 'inc50kbelow_pct', 'inc150kplus_pct', 'medhhinc', 'edavgyrs',\n",
    "                   'ownerocc_pct', 'singleunit_pct', 'unitsavg', 'medyrbuilt', \n",
    "                   'polexposure_pctl', 'polenvt_pctl', 'popsens_pctl',\n",
    "                   'lingisolation_pctl', 'sb535disad_Yes']\n",
    "# 'hhdensity_hhsqkm',\n",
    "# 'hhdensity_fctr_rural', 'hhdensity_fctr_suburban','hhdensity_fctr_urban'\n",
    "# PGE_realdata_encoded.columns[28:48].tolist()\n",
    "\n",
    "# X_PGE_infrademo_demo_cols = X_PGE_infrademo_cols + X_PGE_demo_cols\n",
    "X_PGE_all_cols = X_PGE_infra_cols + X_PGE_service_cols + X_PGE_demo_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR SCE\n",
    "\n",
    "# Split SCE_realdata_encoded according to columns\n",
    "X_all_SCE = SCE_realdata_encoded[X_SCE_all_cols].dropna()\n",
    "y_all_SCE = y_SCE.loc[X_all_SCE.index,:]\n",
    "X_infra_SCE = SCE_realdata_encoded[X_SCE_infra_cols].dropna()\n",
    "y_infra_SCE = y_SCE.loc[X_infra_SCE.index,:]\n",
    "X_service_SCE = SCE_realdata_encoded[X_SCE_service_cols].dropna()\n",
    "y_service_SCE = y_SCE.loc[X_service_SCE.index,:]\n",
    "# X_infrademo_demo_SCE = SCE_realdata_encoded[X_SCE_infrademo_demo_cols].dropna()\n",
    "# y_infrademo_demo_SCE = y_SCE.loc[X_infrademo_demo_SCE.index,:]\n",
    "X_demo_SCE = SCE_realdata_encoded[X_SCE_demo_cols].dropna()\n",
    "y_demo_SCE = y_SCE.loc[X_demo_SCE.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR PG&E\n",
    "\n",
    "# Split PGE_realdata_encoded according to columns\n",
    "X_all_PGE = PGE_realdata_encoded[X_PGE_all_cols].dropna()\n",
    "y_all_PGE = y_PGE.loc[X_all_PGE.index,:]\n",
    "X_infra_PGE = PGE_realdata_encoded[X_PGE_infra_cols].dropna()\n",
    "y_infra_PGE = y_PGE.loc[X_infra_PGE.index,:]\n",
    "X_service_PGE = PGE_realdata_encoded[X_PGE_service_cols].dropna()\n",
    "y_service_PGE = y_PGE.loc[X_service_PGE.index,:]\n",
    "# X_infrademo_demo_PGE = PGE_realdata_encoded[X_PGE_infrademo_demo_cols].dropna()\n",
    "# y_infrademo_demo_PGE = y_PGE.loc[X_infrademo_demo_PGE.index,:]\n",
    "X_demo_PGE = PGE_realdata_encoded[X_PGE_demo_cols].dropna()\n",
    "y_demo_PGE = y_PGE.loc[X_demo_PGE.index,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all_SCE_mod = X_all_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_all_PGE_mod = X_all_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_SCE_mod = X_service_SCE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])\n",
    "X_service_PGE_mod = X_service_PGE.drop(columns = ['Agr_pct', 'Ind_pct', 'Oth_pct'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jennyconde/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:66: RuntimeWarning: invalid value encountered in long_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t1591\n",
      "false positive:\t0\n",
      "true negative:\t28314\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t2616\n",
      "false positive:\t0\n",
      "true negative:\t18607\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t919\n",
      "false positive:\t0\n",
      "true negative:\t28986\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t383\n",
      "false positive:\t0\n",
      "true negative:\t20840\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t474\n",
      "false positive:\t0\n",
      "true negative:\t29431\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t192\n",
      "false positive:\t0\n",
      "true negative:\t21031\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t1395\n",
      "false positive:\t0\n",
      "true negative:\t28510\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t777\n",
      "false positive:\t0\n",
      "true negative:\t20446\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t959\n",
      "false positive:\t0\n",
      "true negative:\t28946\n",
      "\n",
      "nan\n",
      "0.0\n",
      "true positive:\t0\n",
      "false negative:\t592\n",
      "false positive:\t0\n",
      "true negative:\t20631\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for y in y_vals:\n",
    "    # SCE\n",
    "    # All variables\n",
    "    logreg(y, run_vals[0] + '_nohhdensity', X_all_SCE_mod,\n",
    "           y_all_SCE[y], 'SCE', 10)\n",
    "    logreg(y, run_vals[0] + '_nohhdensity', X_all_SCE_mod,\n",
    "           y_all_SCE[y], 'SCE', 1.5)\n",
    "    # Demo variables\n",
    "    logreg(y, run_vals[3] + '_nohhdensity', X_demo_SCE,\n",
    "           y_demo_SCE[y], 'SCE', 10)\n",
    "    logreg(y, run_vals[3] + '_nohhdensity', X_demo_SCE,\n",
    "           y_demo_SCE[y], 'SCE', 1.5)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # PG&E\n",
    "    # All variables\n",
    "    logreg(y, run_vals[0] + '_nohhdensity', X_all_PGE_mod,\n",
    "           y_all_PGE[y], 'PGE', 10)\n",
    "    logreg(y, run_vals[0] + '_nohhdensity', X_all_PGE_mod, \n",
    "           y_all_PGE[y], 'PGE', 1.5)\n",
    "    # Demo variables\n",
    "    logreg(y, run_vals[3] + '_nohhdensity', X_demo_PGE,\n",
    "           y_demo_PGE[y], 'PGE', 10)\n",
    "    logreg(y, run_vals[3] + '_nohhdensity', X_demo_PGE,\n",
    "           y_demo_PGE[y], 'PGE', 1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents: <a class=\"anchor\" id=\"toc_1\"></a>\n",
    "* [Data Importing](#bullet1)\n",
    "* [Data Editing](#bullet2)\n",
    "* [Random Forest Regressions](#bullet3)\n",
    "* [Random Forest Classifications](#bullet4)\n",
    "* [Linear Regression Models](#bullet5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
